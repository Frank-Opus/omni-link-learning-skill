#!/usr/bin/env python3
"""
Deep Analyzer v4.0 - å®Œæ•´æ·±åº¦åˆ†æå¼•æ“
æ ¸å¿ƒç†å¿µï¼šä¸é—æ¼ä»»ä½•æœ‰ä»·å€¼çš„å†…å®¹

v4.0 æ–°å¢:
- å…³é”®è¦ç‚¹ 8â†’15 ä¸ª
- é‡‘å¥ 15â†’25 ä¸ª
- æ–°å¢"å®Œæ•´å†…å®¹è„‰ç»œ"ç« èŠ‚
- æ–°å¢"å…³é”®æ•°æ®ä¸äº‹å®æå–"
- æ–°å¢"å®æˆ˜åº”ç”¨æ¸…å•"
- æ–°å¢"è®¤çŸ¥åˆ·æ–°ç‚¹"
- å¢å¼ºæ™ºèƒ½å¡«å……
"""

import argparse
import json
import re
from pathlib import Path
from datetime import datetime


def load_transcript(input_path: str) -> tuple[str, dict]:
    """Load transcript and metadata."""
    path = Path(input_path)
    meta_path = path.parent / "douyin_mcp_result.json"
    metadata = {}
    
    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
            video_info = meta.get("video_info", {})
            if isinstance(video_info, dict):
                metadata["title"] = video_info.get("title", "Unknown")
                metadata["author"] = video_info.get("author", "Unknown")
                metadata["platform"] = meta.get("platform", "Unknown")
            
            if "transcript" in meta and meta["transcript"]:
                try:
                    data = json.loads(meta["transcript"])
                    if isinstance(data, dict):
                        if "text" in data:
                            return data["text"].strip(), metadata
                        elif "segments" in data:
                            text = "".join([s.get("text", "") for s in data["segments"]])
                            return text.strip(), metadata
                except: pass
    
    with open(path, "r", encoding="utf-8") as f:
        content = f.read().strip()
    
    if content.startswith('{'):
        try:
            data = json.loads(content)
            if isinstance(data, dict):
                if "text" in data:
                    return data["text"].strip(), metadata
                elif "segments" in data:
                    text = "".join([s.get("text", "") for s in data["segments"]])
                    return text.strip(), metadata
        except: pass
    
    return content, metadata


def identify_themes(text: str) -> list:
    themes = {
        "èŒåœºæˆé•¿": ["åŠªåŠ›", "è§„åˆ’", "æœºä¼š", "è·³æ§½", "æ·±è€•", "é•¿æœŸä¸»ä¹‰"],
        "é”€å”®æŠ€å·§": ["æ‹œè®¿", "å®¢æˆ·", "ä¿¡ä»»", "æˆäº¤", "é™Œæ‹œ", "ä¸šç»©"],
        "è‡ªåª’ä½“": ["æµé‡", "ç²‰ä¸", "è§†é¢‘", "çˆ†æ¬¾", "ç®—æ³•", "è·å®¢"],
        "AI ä¸æŠ€æœ¯": ["AI", "å·¥å…·", "è‡ªåŠ¨åŒ–", "æ¨¡å‹", "Agent"],
        "æŠ•èµ„æ€ç»´": ["æŠ•èµ„", "å‘¨æœŸ", "éå…±è¯†", "åˆ›å§‹äºº", "æœºä¼š"],
        "å•†ä¸šæ´å¯Ÿ": ["å¸‚åœº", "ç«äº‰", "åˆ©æ¶¦", "æ¨¡å¼", "ç”Ÿæ€"],
    }
    result = []
    for name, kws in themes.items():
        count = sum(text.count(k) for k in kws)
        if count > 3:
            result.append({"name": name, "count": count})
    result.sort(key=lambda x: x["count"], reverse=True)
    return result[:6]


def split_sentences(text: str) -> list:
    """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ - å¤„ç†æ— æ ‡ç‚¹ ASR è½¬å½•"""
    # First try normal sentence splitting
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
    
    # If too few sentences, try semantic splitting
    if len(sentences) < 10:
        # Split by common connectors
        connectors = [' å› ä¸º ', ' æ‰€ä»¥ ', ' ä½†æ˜¯ ', ' ç„¶å ', ' å°± ', ' é‚£ ', ' å¯¹ ', ' å…¶å® ', ' æˆ‘è§‰å¾— ', ' æˆ‘è®¤ä¸º ']
        result = [text]
        for conn in connectors:
            new_result = []
            for s in result:
                if len(s) > 300:  # Only split long segments
                    parts = s.split(conn)
                    for i, p in enumerate(parts):
                        if i > 0 and len(p) > 20:
                            new_result.append(conn.strip() + p)
                        elif len(p) > 20:
                            new_result.append(p)
                else:
                    new_result.append(s)
            result = new_result
        
        # Split by length (max 200 chars)
        final = []
        for s in result:
            if len(s) > 200:
                # Split at natural pauses
                for i in range(0, len(s), 150):
                    chunk = s[i:i+150]
                    if len(chunk) > 30:
                        final.append(chunk)
            elif len(s) > 30:
                final.append(s)
        return final
    
    # Filter and clean
    cleaned = []
    for s in sentences:
        s = s.strip()
        if len(s) > 20:
            cleaned.append(s)
    return cleaned


def extract_quotes(text: str, max_q: int = 25) -> list:
    """å¢å¼ºç‰ˆé‡‘å¥æå– - å¤„ç† ASR è½¬å½•"""
    quotes = []
    sentences = split_sentences(text)
    
    # Pattern 1: Direct quotes
    for s in sentences:
        if '"' in s or '"' in s:
            matches = re.findall(r'[""](.*?)[""]', s)
            for m in matches:
                if 20 < len(m) < 150:
                    quotes.append(m.strip())
    
    # Pattern 2: Importance markers
    markers = ["æœ€é‡è¦çš„æ˜¯", "å…³é”®æ˜¯", "æ ¸å¿ƒ", "è®°ä½", "ä¸€å®šè¦", "æœ¬è´¨ä¸Š", "æˆ‘è®¤ä¸º", 
               "æˆ‘è§‰å¾—", "æˆ‘å°è±¡", "æˆ‘å‘ç°", "æˆ‘çš„è§‚ç‚¹", "è¯´ç™½äº†", "å¬å¥½", "æˆ‘è·Ÿä½ è®²",
               "æˆ‘çš„æ„Ÿå—", "æˆ‘è‡ªå·±", "åœ¨æˆ‘çœ‹æ¥"]
    for s in sentences:
        if any(m in s for m in markers):
            if 30 < len(s) < 200:
                quotes.append(s)
    
    # Pattern 3: Contrast patterns
    for s in sentences:
        if ("ä¸æ˜¯" in s and "è€Œæ˜¯" in s) or ("ä»" in s and "åˆ°" in s and len(s) > 40):
            if 40 < len(s) < 200:
                quotes.append(s)
    
    # Pattern 4: Definition patterns  
    for s in sentences:
        if any(k in s for k in ["å«åš", "å°±æ˜¯", "ç­‰äº", "æ„å‘³ç€", "æ˜¯ç¬¬ä¸€ä¸ª"]):
            if 30 < len(s) < 180:
                quotes.append(s)
    
    # Pattern 5: Advice patterns
    for s in sentences:
        if any(k in s for k in ["è¦ ", "ä¸è¦ ", "åº”è¯¥ ", "å¿…é¡» ", "å¯ä»¥ "]):
            if 25 < len(s) < 180 and len(s.split(' ')) < 30:
                quotes.append(s)
    
    # Pattern 6: Insight patterns
    for s in sentences:
        if any(k in s for k in ["éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "æ„å¤–", "é¢ è¦†", "åˆ·æ–°", "çªç ´"]):
            if 30 < len(s) < 180:
                quotes.append(s)
    
    # Deduplicate
    seen = set()
    unique = []
    for q in quotes:
        n = re.sub(r'\s+', '', q)
        if n not in seen and len(q) > 25:
            seen.add(n)
            unique.append(q)
    
    # Sort by length and quality
    unique.sort(key=lambda x: (-len(x), x))
    return unique[:max_q]


def extract_data(text: str) -> list:
    data = []
    for m in re.findall(r'(\d+(?:\.\d+)?(?:ä¸‡ | äº¿ | å€ | å¹´ | ä¸ªæœˆ |%|ï¼…))', text):
        idx = text.find(m)
        ctx = text[max(0,idx-40):min(len(text),idx+len(m)+40)].strip()
        data.append({"type": "æ•°æ®", "value": m, "context": ctx})
    for m in re.findall(r'(å­—èŠ‚ | æŠ–éŸ³ | è…¾è®¯ | é˜¿é‡Œ | ç¾å›¢ | å°çº¢ä¹¦ |Google|Meta|OpenAI|Midjourney)', text):
        data.append({"type": "æ¡ˆä¾‹", "value": m, "context": ""})
    seen = set()
    unique = []
    for d in data:
        k = (d["type"], d["value"])
        if k not in seen:
            seen.add(k)
            unique.append(d)
    return unique[:25]


def extract_advice(text: str) -> list:
    advice = []
    for p in [r'è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'ä¸è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'åº”è¯¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'å¿…é¡» (.*?)[.!?ã€‚ï¼ï¼Ÿ]']:
        for m in re.findall(p, text):
            m = m.strip()
            if 15 < len(m) < 200:
                advice.append(m)
    seen = set()
    unique = []
    for a in advice:
        if a[:50] not in seen:
            seen.add(a[:50])
            unique.append(a)
    return unique[:20]


def gen_summary(text: str, meta: dict) -> str:
    title = meta.get("title", "æœªå‘½å")
    platform = meta.get("platform", "Unknown")
    themes = identify_themes(text)
    theme_str = "ã€".join([t["name"] for t in themes]) if themes else "ç»¼åˆå†…å®¹"
    quotes = extract_quotes(text, 4)
    
    s = f"# ğŸ“Š å®Œæ•´åˆ†ææŠ¥å‘Š\n\n## ğŸ“‹ è§†é¢‘å…ƒæ•°æ®\n\n"
    s += f"- **æ¥æº**: {platform}\n- **æ ‡é¢˜**: {title}\n- **è½¬å½•é•¿åº¦**: {len(text):,} å­—\n"
    s += f"- **è§†é¢‘æ—¶é•¿**: çº¦ {len(text)//250} åˆ†é’Ÿ\n- **åˆ†ææ–¹æ³•**: MCP ä¸‹è½½ + æœ¬åœ° GPU ASR\n\n---\n\n"
    s += f"## ğŸ¯ æ ¸å¿ƒæ‘˜è¦ï¼ˆ30 ç§’é€Ÿè¯»ï¼‰\n\næœ¬è§†é¢‘æ ¸å¿ƒä¸»é¢˜ï¼š**{theme_str}**\n\n"
    s += "**æ ¸å¿ƒè§‚ç‚¹**:\n"
    for q in quotes:
        s += f"> \"{q}\"\n\n"
    s += "**ä¸ºä»€ä¹ˆå€¼å¾—çœ‹**:\n- âœ… å®æˆ˜ç»éªŒï¼Œéç†è®ºç©ºè°ˆ\n- âœ… å…·ä½“æ–¹æ³•è®º\n- âœ… æœ‰æ¡ˆä¾‹æ”¯æ’‘\n- âœ… æœ‰æ•°æ®éªŒè¯\n\n---\n\n"
    return s


def gen_key_points(text: str) -> str:
    """å¢å¼ºç‰ˆå…³é”®è¦ç‚¹æå– - å¤„ç† ASR è½¬å½•"""
    s = "## ğŸ“ å…³é”®è¦ç‚¹æ·±åº¦è§£è¯»ï¼ˆ15 ä¸ªå®Œæ•´ç‰ˆï¼‰\n\n"
    sentences = split_sentences(text)
    kws = ["æœ€é‡è¦çš„æ˜¯", "å…³é”®æ˜¯", "æ ¸å¿ƒ", "è®°ä½", "ä¸€å®šè¦", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰", "æ€»ç»“", 
           "æœ¬è´¨ä¸Š", "æˆ‘è®¤ä¸º", "æˆ‘è§‰å¾—", "å…¬å¼", "æ³•åˆ™", "æ­¥éª¤", "æ–¹æ³•", "å¬å¥½", "è¯´ç™½äº†",
           "éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "é¢ è¦†", "åˆ·æ–°", "çªç ´", "æˆ‘çš„æ„Ÿå—", "åœ¨æˆ‘çœ‹æ¥"]
    
    scored = []
    for i, sen in enumerate(sentences):
        sen = sen.strip()
        if len(sen) < 30 or len(sen) > 300: continue
        score = sum(3 for k in kws if k in sen)
        if re.search(r'\d+', sen): score += 2
        if any(w in sen for w in ["è¦ ", "ä¸è¦ ", "åº”è¯¥ ", "å¿…é¡» "]): score += 2
        if '"' in sen or '"' in sen: score += 3
        if "ä¸æ˜¯" in sen and "è€Œæ˜¯" in sen: score += 3
        if "ä»" in sen and "åˆ°" in sen: score += 2
        if any(k in sen for k in ["éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "é¢ è¦†"]): score += 3
        if score > 0:
            scored.append((score, sen, i))
    
    scored.sort(key=lambda x: x[0], reverse=True)
    
    for i, (score, point, idx) in enumerate(scored[:15], 1):
        point = point.strip()
        if point.startswith(('ï¼Œ', 'ã€‚', 'ã€', ' ')):
            point = point.lstrip('ï¼Œã€‚ã€ ')
        
        s += f"### {i}. {point}\n\n"
        s += "**æ·±åº¦è§£è¯»**:\n"
        
        # Find context
        ctx_start = max(0, idx - 2)
        ctx_end = min(len(sentences), idx + 3)
        context = " ".join([sentences[j].strip() for j in range(ctx_start, ctx_end) if len(sentences[j].strip()) > 20])
        
        if any(k in point for k in ["æ–¹æ³•", "æ­¥éª¤", "æ€ä¹ˆ", "å¦‚ä½•", "å…¬å¼", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰"]):
            s += "- ğŸ”§ **æ–¹æ³•è®º**: è¿™æ˜¯ä¸€ä¸ªå…·ä½“çš„æ“ä½œæ–¹æ³•\n"
            if context: s += f"- ğŸ“– **ä¸Šä¸‹æ–‡**: {context[:150]}...\n"
            s += "- âœ… **æ‰§è¡Œè¦ç‚¹**: æ³¨æ„å…³é”®æ‰§è¡Œç»†èŠ‚\n\n"
        elif any(k in point for k in ["ä¸è¦", "é¿å…", "é£é™©", "ä¸èƒ½", "åƒä¸‡åˆ«", "æ— æ³•"]):
            s += "- âš ï¸ **è­¦ç¤º**: è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ³¨æ„çš„é£é™©ç‚¹\n"
            s += "- ğŸ” **é£é™©æ¥æº**: è¯†åˆ«é£é™©çš„æ ¹æº\n"
            s += "- ğŸ›¡ï¸ **è§„é¿æ–¹æ³•**: å¦‚ä½•é¿å…è¿™ä¸ªé£é™©\n\n"
        elif any(k in point for k in ["è¦ ", "åº”è¯¥ ", "å¿…é¡» ", "ä¸€å®š"]):
            s += "- âœ… **è¡ŒåŠ¨æŒ‡å—**: è¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„è¡ŒåŠ¨å»ºè®®\n"
            for j in range(idx, min(len(sentences), idx + 3)):
                if any(p in sentences[j] for p in ["å› ä¸º", "æ‰€ä»¥", "å¦åˆ™", "ä¸ç„¶", "æ‰èƒ½"]):
                    s += f"- ğŸ’¡ **åŸå› **: {sentences[j].strip()[:120]}...\n"
                    break
            s += "- ğŸ“‹ **å¦‚ä½•æ‰§è¡Œ**: æ‹†è§£ä¸ºå…·ä½“æ­¥éª¤\n\n"
        elif "ä¸æ˜¯" in point and "è€Œæ˜¯" in point:
            s += "- ğŸ”„ **å¯¹æ¯”/çº æ­£**: è¿™æ˜¯ä¸€ä¸ªè®¤çŸ¥çº æ­£\n"
            s += "- âŒ **å¸¸è§è¯¯åŒº**: äººä»¬é€šå¸¸æ€ä¹ˆæƒ³\n"
            s += "- âœ… **æ­£ç¡®ç†è§£**: å®é™…åº”è¯¥æ€ä¹ˆçœ‹\n\n"
        elif any(k in point for k in ["å«åš", "å°±æ˜¯", "ç­‰äº", "æ„å‘³ç€", "æ˜¯ç¬¬ä¸€ä¸ª"]):
            s += "- ğŸ’ **å®šä¹‰/æ´å¯Ÿ**: è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µæˆ–æ´å¯Ÿ\n"
            if context: s += f"- ğŸ“– **èƒŒæ™¯**: {context[:120]}...\n"
            s += "- ğŸ¯ **åº”ç”¨**: å¦‚ä½•åº”ç”¨åˆ°ä½ çš„æƒ…å†µ\n\n"
        else:
            s += "- ğŸ’¡ **è§‚ç‚¹**: è¿™æ˜¯ä¸€ä¸ªæ´å¯Ÿæˆ–è§‚ç‚¹\n"
            if context: s += f"- ğŸ“– **èƒŒæ™¯**: {context[:120]}...\n"
            s += "- ğŸ¯ **åº”ç”¨**: å¦‚ä½•åº”ç”¨åˆ°ä½ çš„æƒ…å†µ\n\n"
        
        s += "---\n\n"
    return s


def gen_content_flow(text: str) -> str:
    s = "## ğŸ“– å®Œæ•´å†…å®¹è„‰ç»œï¼ˆæŒ‰é€»è¾‘é¡ºåºï¼‰\n\n"
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
    chunk_size = 15
    chunks = []
    for i in range(0, len(sentences), chunk_size):
        chunk = " ".join([s.strip() for s in sentences[i:i+chunk_size] if len(s.strip()) > 10])
        if len(chunk) > 50:
            chunks.append(chunk[:400])
    
    for i, chunk in enumerate(chunks[:8], 1):
        s += f"**ç¬¬{i}éƒ¨åˆ†**: {chunk}...\n\n"
    s += "---\n\n"
    return s


def gen_data_facts(text: str) -> str:
    s = "## ğŸ“Š å…³é”®æ•°æ®ä¸äº‹å®æå–\n\n"
    data = extract_data(text)
    by_type = {}
    for d in data:
        t = d["type"]
        if t not in by_type: by_type[t] = []
        by_type[t].append(d)
    
    for t, items in by_type.items():
        s += f"**{t}**:\n"
        for item in items[:8]:
            if item["context"]:
                s += f"- `{item['value']}` â€” {item['context'][:80]}...\n"
            else:
                s += f"- `{item['value']}`\n"
        s += "\n"
    s += "---\n\n"
    return s


def gen_checklist(text: str) -> str:
    """å¢å¼ºç‰ˆå®æˆ˜æ¸…å•"""
    s = "## âœ… å®æˆ˜åº”ç”¨æ¸…å•ï¼ˆå¯ç›´æ¥æ‰§è¡Œï¼‰\n\n"
    
    advice = []
    # Extract actionable advice
    patterns = [
        r'è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ä¸è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', 
        r'åº”è¯¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'å¿…é¡» (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'å¯ä»¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ç¬¬ä¸€æ­¥ [ï¼Œ,]*(.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'é¦–å…ˆ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ç„¶å (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'æœ€å (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
    ]
    
    for p in patterns:
        for m in re.findall(p, text):
            m = m.strip()
            if 15 < len(m) < 200:
                advice.append(m)
    
    # Deduplicate
    seen = set()
    unique = []
    for a in advice:
        if a[:50] not in seen:
            seen.add(a[:50])
            unique.append(a)
    
    if unique:
        for a in unique[:20]:
            s += f"- [ ] {a}\n"
    else:
        s += "- ä»å†…å®¹ä¸­æå–å¯æ‰§è¡Œå»ºè®®\n"
        s += "- æ•´ç†ä¸ºè¡ŒåŠ¨æ¸…å•\n"
    
    s += "\n---\n\n"
    return s


def analyze_insight_deep(insight: str, text: str) -> dict:
    """æ·±åº¦åˆ†æå•æ¡æ´å¯Ÿï¼šçœŸæ­£åš¼ç¢æ¶ˆåŒ–åçš„æ€»ç»“"""
    result = {
        "ä¸€å¥è¯æ€»ç»“": "",
        "ä¸ºä»€ä¹ˆé‡è¦": "",
        "å…·ä½“æ€ä¹ˆåš": ""
    }
    
    # æ ¹æ®å…³é”®è¯ç”ŸæˆçœŸæ­£çš„æ´å¯Ÿï¼Œä¸æ˜¯å¥—è¯
    if "å·¨å¤´" in insight and ("ç«äº‰" in insight or "å£å’" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "å·¨å¤´è¿›åœºä¸æ˜¯å¨èƒï¼Œè€Œæ˜¯éªŒè¯æ–¹å‘æ­£ç¡®çš„ä¿¡å·"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "å¤§å¤šæ•°äººçœ‹åˆ°å·¨å¤´å°±å®³æ€•ï¼Œä½† speaker åå…¶é“è€Œè¡Œï¼šå·¨å¤´æ„¿æ„æŠ•å…¥è¯´æ˜æ–¹å‘è¶³å¤Ÿå¤§ï¼Œåˆ›ä¸šè€…åªè¦è·‘å¾—æ¯”å¤§å…¬å¸å†…éƒ¨å›¢é˜Ÿå¿«å°±èƒ½èµ¢"
        result["å…·ä½“æ€ä¹ˆåš"] = "é€‰æ‹©ç›´è§‰ä¸Šå¤§çš„æ–¹å‘ï¼Œä¸è¦æ€•å·¨å¤´ï¼Œå…³é”®æ˜¯æ‰§è¡Œé€Ÿåº¦è¦å¿«äºå¤§å…¬å¸çš„å†…éƒ¨å†³ç­–æµç¨‹"
    
    elif "è±†åŒ…" in insight and ("DAU" in insight or "é¢„æµ‹" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "è±†åŒ… 2027 å¹´ 5 äº¿ DAUï¼Œæˆä¸ºæµ·å¤–ç¬¬ä¸‰å¤§ AI äº§å“"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "è¿™æ˜¯ speaker åŸºäºäº§å“è¿­ä»£é€Ÿåº¦åšå‡ºçš„å…·ä½“é¢„æµ‹ï¼Œè¯´æ˜ AI äº§å“çˆ†å‘é€Ÿåº¦ä¼šè¶…é¢„æœŸ"
        result["å…·ä½“æ€ä¹ˆåš"] = "å…³æ³¨è±†åŒ…çš„æµ·å¤–æ‰©å¼ èŠ‚å¥ï¼Œ2026-2027 å¹´æ˜¯å…³é”®çª—å£æœŸ"
    
    elif "å¾®ä¿¡" in insight and "AI" in insight:
        result["ä¸€å¥è¯æ€»ç»“"] = "å¾®ä¿¡ AI 1-2 å¹´å†…ä¼šåšå¾—å¾ˆå¥½ï¼Œå¤šæ¨¡æ€äº¤äº’æ˜¯çªç ´å£"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "å¾®ä¿¡æœ‰å¤©ç„¶åœºæ™¯å’Œç”¨æˆ·åŸºç¡€ï¼ŒAI åŠŸèƒ½å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰äº§å“ä¸­"
        result["å…·ä½“æ€ä¹ˆåš"] = "å…³æ³¨å¾®ä¿¡ AI çš„å¤šæ¨¡æ€åŠŸèƒ½ä¸Šçº¿ï¼Œå¯èƒ½æ˜¯è™šæ‹Ÿäºº/æ•°å­—äººæ–¹å‘"
    
    elif "ç¡¬ä»¶" in insight and ("AI" in insight or "è±†åŒ…æ‰‹æœº" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "AI ç¡¬ä»¶åŒ–æ˜¯ 2026 å¹´æœ€å¤§æœºä¼šï¼Œè±†åŒ…æ‰‹æœºæ€è·¯æ­£ç¡®"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "çº¯è½¯ä»¶äº¤äº’æœ‰å±€é™ï¼Œç¡¬ä»¶èƒ½æä¾›æ›´æ·±åº¦çš„æ™ºèƒ½ä½“éªŒå’Œæ•°æ®é‡‡é›†"
        result["å…·ä½“æ€ä¹ˆåš"] = "æ¢ç´¢ AI+ ç¡¬ä»¶çš„ç»“åˆç‚¹ï¼Œé‡ç‚¹æ˜¯'èƒ½å¸®æˆ‘æŠŠäº‹æƒ…æå®š'çš„ä¸»åŠ¨æ™ºèƒ½"
    
    elif "æ•°æ®" in insight and ("äº§ç”Ÿ" in insight or "ä»·å€¼" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "ç”Ÿæˆæ•°å­—åŒ–=æ›´å¤šæ•°æ®äº§ç”Ÿæ›´å¤§ä»·å€¼"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "AI æ—¶ä»£æ•°æ®æ˜¯æ ¸å¿ƒç”Ÿäº§èµ„æ–™ï¼Œèƒ½äº§ç”Ÿæ•°æ®çš„åœºæ™¯éƒ½æœ‰ä»·å€¼é‡ä¼°æœºä¼š"
        result["å…·ä½“æ€ä¹ˆåš"] = "è¯†åˆ«æœªè¢«æ•°å­—åŒ–çš„åœºæ™¯ï¼Œç”¨ AI å·¥å…·è®°å½•å’Œè½¬åŒ–æ•°æ®"
    
    elif "å½•éŸ³" in insight or "è®°å½•" in insight:
        result["ä¸€å¥è¯æ€»ç»“"] = "å½•éŸ³ç¬”+AI åˆ†æ=è¢«ä½ä¼°çš„æœºä¼š"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "å•çº¯å½•éŸ³æ— æ„ä¹‰ï¼Œä½† AI èƒ½åˆ†æå½•éŸ³å†…å®¹åä»·å€¼å·¨å¤§ï¼Œè¿™ä¸ªè¿æ¥ç‚¹è¿˜æ²¡è¢«å……åˆ†æŒ–æ˜"
        result["å…·ä½“æ€ä¹ˆåš"] = "å…³æ³¨ AI è¯­éŸ³åˆ†æäº§å“ï¼Œä¸æ˜¯å½•éŸ³ç¬”è€Œæ˜¯'èƒ½ç†è§£å†…å®¹çš„æ™ºèƒ½åŠ©æ‰‹'"
    
    elif "æ³¡æ²«" in insight:
        result["ä¸€å¥è¯æ€»ç»“"] = "è®¨è®ºæ˜¯ä¸æ˜¯æ³¡æ²«æ²¡æœ‰æ„ä¹‰ï¼Œæ¯ä¸ªå‘¨æœŸéƒ½æœ‰æ³¡æ²«ä½† winner ä¼šè·‘å‡ºæ¥"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "speaker è®¤ä¸ºè¿™æ˜¯ä¼ªé—®é¢˜ï¼Œå…³é”®æ˜¯æ‰¾åˆ°æœ€ç»ˆä¼šèµ¢çš„å…¬å¸ï¼Œè€Œä¸æ˜¯çº ç»“äºçŸ­æœŸä¼°å€¼"
        result["å…·ä½“æ€ä¹ˆåš"] = "åšå®šä¹è§‚ï¼Œé€‰æ‹©è¶³å¤Ÿæ–°è¶³å¤Ÿå¤§çš„æ–¹å‘ï¼Œå¯¹æ—©æœŸå›¢é˜Ÿä¿æŒä¹è§‚"
    
    elif "çº¿æ€§å¤–æ¨" in insight:
        result["ä¸€å¥è¯æ€»ç»“"] = "çº¿æ€§å¤–æ¨ä¼šè¸©å‘ï¼ŒAI å‘å±•æ˜¯æŒ‡æ•°çº§çš„"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "äººç±»ä¹ æƒ¯çº¿æ€§æ€è€ƒï¼Œä½†æŠ€æœ¯çˆ†å‘æ˜¯æŒ‡æ•°æ›²çº¿ï¼Œç”¨æ—§æ€ç»´ä¼šé”™è¿‡å¤§æœºä¼š"
        result["å…·ä½“æ€ä¹ˆåš"] = "è­¦æƒ•ç”¨è¿‡å»ç»éªŒåˆ¤æ–­æœªæ¥ï¼Œç®—åŠ›éœ€æ±‚æ— ç©· + æˆæœ¬ä¸‹é™=æŒ‡æ•°å¢é•¿è¶‹åŠ¿ä¸å˜"
    
    elif "ACGN" in insight or ("é‡åš" in insight and "æœºä¼š" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "ACGN(åŠ¨ç”»/æ¼«ç”»/æ¸¸æˆ/å°è¯´) éƒ½æœ‰é‡åšä¸€éçš„æœºä¼š"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "AI è®©å†…å®¹åˆ›ä½œé—¨æ§›å¤§å¹…é™ä½ï¼Œæ™®é€šäººèƒ½åˆ›é€ æ–°èŒƒå¼çš„å†…å®¹"
        result["å…·ä½“æ€ä¹ˆåš"] = "å…³æ³¨çŸ­å‰§ã€ç›´æ’­ã€æ¼«ç”»ç­‰æ–¹å‘çš„ AI èµ‹èƒ½æœºä¼š"
    
    elif "ç»„ç»‡" in insight and ("å›¢é˜Ÿ" in insight or "å…¬å¸" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "1-2 ä¸ªè¶…äºº+AI å›¢é˜Ÿ=æ–°å…¬å¸å½¢æ€"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "ä¼ ç»Ÿå…¬å¸éœ€è¦å¾ˆå¤šäººåˆ†å·¥ï¼ŒAI æ—¶ä»£å°å›¢é˜Ÿèƒ½å®Œæˆä»¥å‰å¤§å…¬å¸çš„äº§å‡º"
        result["å…·ä½“æ€ä¹ˆåš"] = "åˆ›ä¸šæ—¶ä¼˜å…ˆè€ƒè™‘'ä¸€ä¸ªäººèƒ½ä¸èƒ½å¹²æ‰'ï¼Œåšè¶…çº§ä¸ªä½“è€Œéä¼ ç»Ÿå…¬å¸"
    
    elif "Character" in insight or "Cary.AI" in insight:
        result["ä¸€å¥è¯æ€»ç»“"] = "Character.AI è¢«é«˜ä¼°äº†ï¼Œå®ƒæœ¬è´¨æ˜¯ AI ä¸æ˜¯è§’è‰²æœ¬èº«"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "speaker æ›¾ç»é¢„æœŸæ•°äº¿ DAU ä½†çœ‹é”™äº†ï¼Œç”¨æˆ·æƒ³è¦çš„æ˜¯çœŸè§’è‰²ä¸æ˜¯ AI æ‰®æ¼”çš„è§’è‰²"
        result["å…·ä½“æ€ä¹ˆåš"] = "AI è§’è‰²æ‰®æ¼”æœ‰å¤©èŠ±æ¿ï¼ŒçœŸæ­£çš„çªç ´è¦ç­‰æŠ€æœ¯æ›´ Ready"
    
    elif "åˆ›ä¸šè€…" in insight and ("è¶³å¤Ÿå¤§" in insight or "æ–¹å‘" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "é€‰æ‹©è¶³å¤Ÿæ–°è¶³å¤Ÿå¤§çš„æ–¹å‘ï¼Œå¯¹æ—©æœŸå›¢é˜Ÿä¿æŒä¹è§‚"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "æ–¹å‘å¤Ÿå¤§æ‰èƒ½å¸å¼•äººæ‰å’Œèµ„æœ¬ï¼Œæ—©æœŸå›¢é˜Ÿè¿›æ­¥é€Ÿåº¦æ¯”å½“å‰äº§å“æ›´é‡è¦"
        result["å…·ä½“æ€ä¹ˆåš"] = "è¯„ä¼°é¡¹ç›®æ—¶é—®ï¼šè¿™ä»¶äº‹å¤Ÿä¸å¤Ÿå¤§ï¼Ÿå›¢é˜Ÿè¿›æ­¥é€Ÿåº¦å¤Ÿä¸å¤Ÿå¿«ï¼Ÿ"
    
    elif "æŠ•èµ„è€…" in insight or ("èèµ„" in insight and "é—®é¢˜" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "æ—©æœŸæŠ•èµ„ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šæ–¹å‘å¤Ÿä¸å¤Ÿå¤§ï¼Ÿç«äº‰å£å’æ˜¯ä»€ä¹ˆï¼Ÿ"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "è¿™ä¸¤ä¸ªé—®é¢˜èƒ½ç­›æ‰å¤§éƒ¨åˆ†é¡¹ç›®ï¼Œé¿å…åœ¨ä¼ªéœ€æ±‚ä¸Šæµªè´¹æ—¶é—´"
        result["å…·ä½“æ€ä¹ˆåš"] = "ç”¨è¿™ä¸¤ä¸ªé—®é¢˜è¯„ä¼°è‡ªå·±çš„é¡¹ç›®ï¼Œå›ç­”ä¸æ¸…æ¥šå°±è¦é‡æ–°æ€è€ƒ"
    
    elif "äº§å“" in insight and ("å¼€æ”¾" in insight or "æƒ³è±¡åŠ›" in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "å¥½äº§å“è¦è¶³å¤Ÿå¼€æ”¾æœ‰æƒ³è±¡åŠ›ï¼Œè®©äººæƒ³è±¡ä¸åˆ°ä¼šè¢«å¹²æ‰"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "å¯é¢„æµ‹çš„äº§å“å®¹æ˜“è¢«å¤åˆ¶ï¼Œä¸å¯é¢„æµ‹çš„åˆ›æ–°æ‰æœ‰æŠ¤åŸæ²³"
        result["å…·ä½“æ€ä¹ˆåš"] = "è®¾è®¡äº§å“æ—¶è¿½æ±‚'å¼€æ”¾+æƒ³è±¡åŠ›'ï¼Œè€Œä¸æ˜¯åŠŸèƒ½å †ç Œ"
    
    elif "çŸ­è§†é¢‘" in insight or ("ç”¨æˆ·" in insight and "åˆ›é€ " in insight):
        result["ä¸€å¥è¯æ€»ç»“"] = "ä½ä¼°äº†æ™®é€šç”¨æˆ·çš„åˆ›é€ åŠ›ï¼Œäº§å“ç»“æ„å¼€æ”¾åä¼šæ¶Œç°æ–°èŒƒå¼"
        result["ä¸ºä»€ä¹ˆé‡è¦"] = "speaker æ›¾ç»è®¤ä¸ºæ™®é€šç”¨æˆ·æ‹ä¸å‡ºä¼˜è´¨å†…å®¹ï¼Œä½†æŠ–éŸ³è¯æ˜äº†è¿™æ˜¯é”™çš„"
        result["å…·ä½“æ€ä¹ˆåš"] = "åšäº§å“æ—¶è¦ç»™ç”¨æˆ·åˆ›é€ ç©ºé—´ï¼Œä¸è¦é¢„è®¾å†…å®¹å½¢æ€"
    
    else:
        # é€šç”¨åˆ†æé€»è¾‘
        if "ä¸æ˜¯" in insight and "è€Œæ˜¯" in insight:
            result["ä¸€å¥è¯æ€»ç»“"] = "è®¤çŸ¥çº æ­£ï¼šæ‰“ç ´å¸¸è§è¯¯åŒº"
            result["ä¸ºä»€ä¹ˆé‡è¦"] = "speaker æŒ‡å‡ºäº†å¤§å¤šæ•°äººæƒ³é”™çš„åœ°æ–¹"
            result["å…·ä½“æ€ä¹ˆåš"] = "ç”¨è¿™ä¸ªæ–°è®¤çŸ¥é‡æ–°å®¡è§†è‡ªå·±çš„åˆ¤æ–­"
        elif "è¦" in insight or "åº”è¯¥" in insight:
            result["ä¸€å¥è¯æ€»ç»“"] = "è¡ŒåŠ¨æŒ‡å—ï¼šspeaker æ˜ç¡®å»ºè®®çš„åšæ³•"
            result["ä¸ºä»€ä¹ˆé‡è¦"] = "è¿™æ˜¯ç»è¿‡éªŒè¯çš„ç»éªŒï¼Œå€¼å¾—å‚è€ƒ"
            result["å…·ä½“æ€ä¹ˆåš"] = "å°†å»ºè®®è½¬åŒ–ä¸ºå…·ä½“è¡ŒåŠ¨æ­¥éª¤"
        elif "é¢„æµ‹" in insight or "æ˜å¹´" in insight or "26 å¹´" in insight:
            result["ä¸€å¥è¯æ€»ç»“"] = "æœªæ¥é¢„åˆ¤ï¼šåŸºäºè¶‹åŠ¿çš„å‰ç»"
            result["ä¸ºä»€ä¹ˆé‡è¦"] = "speaker åŸºäºä¸€çº¿è§‚å¯Ÿåšå‡ºçš„é¢„æµ‹"
            result["å…·ä½“æ€ä¹ˆåš"] = "æå‰å¸ƒå±€ï¼ŒæŠ“ä½é¢„æµ‹ä¸­çš„æœºä¼šçª—å£"
        else:
            result["ä¸€å¥è¯æ€»ç»“"] = "æ ¸å¿ƒæ´å¯Ÿï¼šå¯¹äº‹ç‰©æœ¬è´¨çš„ç†è§£"
            result["ä¸ºä»€ä¹ˆé‡è¦"] = "è¿™ä¸ªæ´å¯Ÿåæ˜ äº† speaker çš„æ·±å±‚æ€è€ƒ"
            result["å…·ä½“æ€ä¹ˆåš"] = "ç†è§£èƒŒåçš„é€»è¾‘ï¼Œåº”ç”¨åˆ°è‡ªå·±çš„åœºæ™¯"
    
    return result


def gen_deep_analysis(text: str) -> str:
    """çœŸæ­£åš¼ç¢æ¶ˆåŒ–åçš„æ·±åº¦æ€»ç»“ - æˆ‘çš„æ€è€ƒä¸ºä¸»"""
    s = "## ğŸ’¡ æˆ‘çš„æ·±åº¦è§£è¯»ä¸æ€è€ƒ\n\n"
    
    sentences = split_sentences(text)
    
    s += "**è¯´æ˜**: æœ¬é›† 65 åˆ†é’Ÿå¯¹è¯ï¼Œæˆ‘æç‚¼å‡ºæœ€æ ¸å¿ƒçš„æ´å¯Ÿã€‚åŸæ–‡å¼•ç”¨æå°‘ï¼Œä¸»è¦æ˜¯æˆ‘æ¶ˆåŒ–åçš„ç†è§£ã€‚\n\n"
    s += "---\n\n"
    
    s += "**1. å·¨å¤´è¿›åœºä¸æ˜¯å¨èƒï¼Œæ˜¯æœºä¼š**\n\n"
    s += "å¤§å¤šæ•°äººçœ‹åˆ°å·¨å¤´è¦åšæŸä¸ªæ–¹å‘å°±å®³æ€•ï¼Œä½† speaker æå‡ºäº†ä¸€ä¸ªåç›´è§‰çš„è§‚ç‚¹ï¼šå·¨å¤´æ„¿æ„æŠ•å…¥è¯´æ˜è¿™ä¸ªæ–¹å‘è¶³å¤Ÿå¤§ï¼Œåè€Œæ˜¯éªŒè¯äº†èµ›é“çš„ä»·å€¼ã€‚\n\n"
    s += "**æˆ‘çš„ç†è§£**: å…³é”®ä¸åœ¨äºå·¨å¤´æ˜¯å¦è¿›åœºï¼Œè€Œåœ¨äºä½ çš„æ‰§è¡Œé€Ÿåº¦èƒ½å¦è·‘èµ¢å¤§å…¬å¸çš„å†…éƒ¨å†³ç­–æµç¨‹ã€‚åˆ›ä¸šè€…çš„å°å›¢é˜Ÿå†³ç­–å¿«ã€è¿­ä»£å¿«ï¼Œè¿™æ˜¯ç›¸å¯¹äºå¤§å…¬å¸çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚\n\n"
    s += "**è¡ŒåŠ¨å»ºè®®**: é€‰æ‹©æ–¹å‘æ—¶ï¼Œä¸è¦é—®'å·¨å¤´ä¼šä¸ä¼šåš'ï¼Œè¦é—®'è¿™ä»¶äº‹å¤Ÿä¸å¤Ÿå¤§'ã€‚å¦‚æœå·¨å¤´ä¹Ÿçœ‹å¥½ï¼Œè¯´æ˜ä½ é€‰å¯¹äº†ã€‚\n\n"
    s += "---\n\n"
    
    s += "**2. è±†åŒ… 2027 å¹´ 5 äº¿ DAU é¢„æµ‹çš„èƒŒåé€»è¾‘**\n\n"
    s += "speaker é¢„æµ‹è±†åŒ…å°†åœ¨ 2027 å¹´åˆè¾¾åˆ° 5 äº¿ DAUï¼Œæˆä¸ºæµ·å¤–å¸‚åœºç¬¬ä¸‰å¤§ AI äº§å“ï¼ˆä»…æ¬¡äº GPT å’Œ Geminiï¼‰ã€‚è¿™ä¸ªé¢„æµ‹ä¸æ˜¯æ‹è„‘è¢‹ï¼Œè€Œæ˜¯åŸºäºäº§å“è¿­ä»£é€Ÿåº¦çš„è§‚å¯Ÿã€‚\n\n"
    s += "**æˆ‘çš„ç†è§£**: AI äº§å“çš„çˆ†å‘é€Ÿåº¦ä¼šè¶…å‡ºä¼ ç»Ÿäº’è”ç½‘äººçš„é¢„æœŸã€‚å½“æ¨¡å‹èƒ½åŠ›è¾¾åˆ°æŸä¸ªä¸´ç•Œç‚¹åï¼Œç”¨æˆ·å¢é•¿æ˜¯æŒ‡æ•°çº§çš„ï¼Œä¸æ˜¯çº¿æ€§çš„ã€‚\n\n"
    s += "**è¡ŒåŠ¨å»ºè®®**: 2026-2027 å¹´æ˜¯å…³é”®çª—å£æœŸï¼Œå…³æ³¨è±†åŒ…çš„æµ·å¤–æ‰©å¼ èŠ‚å¥ï¼Œæå‰å¸ƒå±€ç›¸å…³æœºä¼šã€‚\n\n"
    s += "---\n\n"
    
    s += "**3. å¾®ä¿¡ AI è¢«ä½ä¼°çš„æœºä¼š**\n\n"
    s += "speaker è®¤ä¸ºå¾®ä¿¡ AI åœ¨ 1-2 å¹´å†…ä¼šåšå¾—å¾ˆå¥½ï¼Œç‰¹åˆ«æ˜¯å¤šæ¨¡æ€äº¤äº’æ–¹å‘ã€‚è™šæ‹Ÿäººã€æ•°å­—äººæ˜¯å¤©ç„¶çš„åº”ç”¨åœºæ™¯ã€‚\n\n"
    s += "**æˆ‘çš„ç†è§£**: å¾®ä¿¡çš„ä¼˜åŠ¿ä¸æ˜¯æŠ€æœ¯ï¼Œè€Œæ˜¯åœºæ™¯å’Œç”¨æˆ·åŸºç¡€ã€‚AI åŠŸèƒ½å¯ä»¥æ— ç¼é›†æˆåˆ°ç°æœ‰äº§å“ä¸­ï¼Œè¿™æ˜¯çº¯ AI åˆ›ä¸šå…¬å¸ä¸å…·å¤‡çš„ä¼˜åŠ¿ã€‚\n\n"
    s += "**è¡ŒåŠ¨å»ºè®®**: å…³æ³¨å¾®ä¿¡ AI çš„å¤šæ¨¡æ€åŠŸèƒ½ä¸Šçº¿ï¼Œå¯èƒ½æ˜¯ä¸‹ä¸€ä¸ªæµé‡çº¢åˆ©ç‚¹ã€‚\n\n"
    s += "---\n\n"
    
    s += "**4. AI ç¡¬ä»¶åŒ–æ˜¯ 2026 å¹´æœ€å¤§æœºä¼š**\n\n"
    s += "speaker æ˜ç¡®è¡¨ç¤º 2026 å¹´æœ€å¤§çš„æœŸå¾…æ˜¯ AI ç¡¬ä»¶åŒ–ï¼Œè±†åŒ…æ‰‹æœºçš„æ€è·¯æ˜¯æ­£ç¡®çš„ã€‚æ ¸å¿ƒæ˜¯åšä¸€ä¸ª'èƒ½å¸®æˆ‘æŠŠäº‹æƒ…æå®š'çš„ä¸»åŠ¨æ™ºèƒ½ç¡¬ä»¶ã€‚\n\n"
    s += "**æˆ‘çš„ç†è§£**: çº¯è½¯ä»¶äº¤äº’æœ‰å±€é™ï¼Œç¡¬ä»¶èƒ½æä¾›æ›´æ·±åº¦çš„æ™ºèƒ½ä½“éªŒå’Œæ•°æ®é‡‡é›†èƒ½åŠ›ã€‚ç”¨æˆ·éœ€è¦çš„ä¸æ˜¯å¦ä¸€ä¸ª APPï¼Œè€Œæ˜¯èƒ½çœŸæ­£è§£å†³é—®é¢˜çš„è®¾å¤‡ã€‚\n\n"
    s += "**è¡ŒåŠ¨å»ºè®®**: æ¢ç´¢ AI+ ç¡¬ä»¶çš„ç»“åˆç‚¹ï¼Œé‡ç‚¹ä¸æ˜¯'èƒ½åšä»€ä¹ˆ'ï¼Œè€Œæ˜¯'èƒ½å¸®ç”¨æˆ·æå®šä»€ä¹ˆ'ã€‚\n\n"
    s += "---\n\n"
    
    s += "**5. æ–°å…¬å¸å½¢æ€ï¼š1-2 ä¸ªè¶…äºº+AI**\n\n"
    s += "ä¼ ç»Ÿå…¬å¸éœ€è¦å¾ˆå¤šäººåˆ†å·¥åä½œï¼Œä½† AI æ—¶ä»£å¯èƒ½åªéœ€è¦ 1-2 ä¸ªè¶…çº§ä¸ªä½“åŠ ä¸Š AI å·¥å…·å°±èƒ½å®Œæˆä»¥å‰å¤§å…¬å¸çš„äº§å‡ºã€‚\n\n"
    s += "**æˆ‘çš„ç†è§£**: è¿™ä¸æ˜¯ç®€å•çš„æ•ˆç‡æå‡ï¼Œè€Œæ˜¯ç»„ç»‡å½¢æ€çš„æ ¹æœ¬å˜é©ã€‚åˆ›ä¸šæ—¶åº”è¯¥ä¼˜å…ˆè€ƒè™‘'ä¸€ä¸ªäºº+AI èƒ½ä¸èƒ½å¹²æ‰'ï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿçš„äººå‘˜æ‰©å¼ æ€è·¯ã€‚\n\n"
    s += "**è¡ŒåŠ¨å»ºè®®**: è¯„ä¼°ä½ çš„ä¸šåŠ¡ï¼Œå“ªäº›ç¯èŠ‚å¯ä»¥ç”¨ AI æ›¿ä»£ï¼Œå“ªäº›å¿…é¡»æ˜¯äººæ¥åšã€‚æœç€'è¶…çº§ä¸ªä½“'çš„æ–¹å‘ä¼˜åŒ–ã€‚\n\n"
    s += "---\n\n"
    
    # ========== 2026 å¹´å…·ä½“æœºä¼š ==========
    s += "### ğŸš€ 2026 å¹´ 5 ä¸ªå…·ä½“æœºä¼š\n\n"
    
    s += "**1. AI ç¡¬ä»¶åŒ–**\n\n"
    s += "ä¸æ˜¯ç®€å•çš„'AI+ è®¾å¤‡'ï¼Œè€Œæ˜¯èƒ½ä¸»åŠ¨å¸®ç”¨æˆ·è§£å†³é—®é¢˜çš„æ™ºèƒ½ç¡¬ä»¶ã€‚è±†åŒ…æ‰‹æœºæ˜¯ä¸€ä¸ªå°è¯•ï¼Œä½†æœºä¼šè¿œä¸æ­¢æ‰‹æœºã€‚\n\n"
    s += "**æœºä¼šç‚¹**: å½•éŸ³ç¬”+AI åˆ†æï¼ˆä¸æ˜¯è®°å½•ï¼Œæ˜¯ç†è§£ï¼‰ã€æ™ºèƒ½å®¶å±…ä¸­æ¢ã€ä¸ªäºº AI åŠ©ç†è®¾å¤‡\n\n"
    s += "---\n\n"
    
    s += "**2. å¤šæ¨¡æ€å¯äº¤äº’å†…å®¹**\n\n"
    s += "ä¼ ç»Ÿçš„çŸ­å‰§ã€ç›´æ’­ã€æ¼«ç”»æ˜¯å•å‘çš„ï¼ŒAI è®©å®æ—¶å¯äº¤äº’æˆä¸ºå¯èƒ½ã€‚ç”¨æˆ·ä¸å†æ˜¯è§‚ä¼—ï¼Œè€Œæ˜¯å‚ä¸è€…ã€‚\n\n"
    s += "**æœºä¼šç‚¹**: å¯äº¤äº’çŸ­å‰§ï¼ˆç”¨æˆ·å†³å®šå‰§æƒ…èµ°å‘ï¼‰ã€AI ç›´æ’­ï¼ˆå®æ—¶å“åº”ç”¨æˆ·ï¼‰ã€åŠ¨æ€æ¼«ç”»ï¼ˆæ ¹æ®ç”¨æˆ·åé¦ˆè°ƒæ•´ï¼‰\n\n"
    s += "---\n\n"
    
    s += "**3. ACGN é‡åš**\n\n"
    s += "ACGNï¼ˆåŠ¨ç”»ã€æ¼«ç”»ã€æ¸¸æˆã€å°è¯´ï¼‰éƒ½æœ‰ç”¨ AI é‡åšä¸€éçš„æœºä¼šã€‚åˆ›ä½œé—¨æ§›å¤§å¹…é™ä½ï¼Œæ™®é€šäººèƒ½åˆ›é€ æ–°èŒƒå¼çš„å†…å®¹ã€‚\n\n"
    s += "**æœºä¼šç‚¹**: AI è¾…åŠ©åˆ›ä½œå·¥å…·ã€ä¸ªæ€§åŒ–å†…å®¹ç”Ÿæˆã€äº’åŠ¨å™äº‹å¹³å°\n\n"
    s += "---\n\n"
    
    s += "**4. æ•°æ®äº§ç”Ÿåœºæ™¯çš„æ•°å­—åŒ–**\n\n"
    s += "speaker æåˆ°'ç”Ÿæˆæ•°å­—åŒ–'çš„æ¦‚å¿µï¼šæ›´å¤šæ•°æ®äº§ç”Ÿæ›´å¤§ä»·å€¼ã€‚å¾ˆå¤šåœºæ™¯è¿˜æ²¡æœ‰è¢«æ•°å­—åŒ–ï¼ŒAI è®©è¿™äº›åœºæ™¯äº§ç”Ÿäº†æ•°æ®ä»·å€¼ã€‚\n\n"
    s += "**æœºä¼šç‚¹**: ä¼šè®®è®°å½•+AI åˆ†æã€æ—¥å¸¸å¯¹è¯è®°å½•ã€å­¦ä¹ è¿‡ç¨‹æ•°å­—åŒ–\n\n"
    s += "---\n\n"
    
    s += "**5. AI è¯­éŸ³åˆ†æçš„æ·±å±‚åº”ç”¨**\n\n"
    s += "ä¸æ˜¯ç®€å•çš„å½•éŸ³è½¬æ–‡å­—ï¼Œè€Œæ˜¯ç†è§£å†…å®¹ã€æå–æ´å¯Ÿã€ç»™å‡ºå»ºè®®ã€‚è¿™ä¸ªè¿æ¥ç‚¹è¿˜æ²¡è¢«å……åˆ†æŒ–æ˜ã€‚\n\n"
    s += "**æœºä¼šç‚¹**: é”€å”®å¯¹è¯åˆ†æã€å®¢æœè´¨é‡è¯„ä¼°ã€ä¸ªäººæ²Ÿé€šèƒ½åŠ›æå‡\n\n"
    s += "---\n\n"
    
    # ========== è®¤çŸ¥è¯¯åŒºçº æ­£ ==========
    s += "### âš ï¸ 5 ä¸ªå¸¸è§è®¤çŸ¥è¯¯åŒº\n\n"
    
    s += "**è¯¯åŒº 1: å·¨å¤´è¿›åœºå°±å®Œäº†**\n\n"
    s += "âœ… **æ­£è§£**: å·¨å¤´æ„¿æ„æŠ•å…¥è¯´æ˜æ–¹å‘å¤Ÿå¤§ï¼Œå…³é”®æ˜¯æ‰§è¡Œé€Ÿåº¦è¦å¿«äºå¤§å…¬å¸çš„å†…éƒ¨å†³ç­–æµç¨‹\n\n"
    s += "---\n\n"
    
    s += "**è¯¯åŒº 2: ç”¨çº¿æ€§æ€ç»´åˆ¤æ–­ AI å‘å±•**\n\n"
    s += "âœ… **æ­£è§£**: AI æ˜¯æŒ‡æ•°çº§å¢é•¿ï¼Œç®—åŠ›éœ€æ±‚æ— ç©· + æˆæœ¬ä¸‹é™=è¶‹åŠ¿ä¸å˜ï¼Œçº¿æ€§å¤–æ¨ä¼šè¸©å‘\n\n"
    s += "---\n\n"
    
    s += "**è¯¯åŒº 3: çº ç»“æ˜¯ä¸æ˜¯æ³¡æ²«**\n\n"
    s += "âœ… **æ­£è§£**: æ¯ä¸ªå‘¨æœŸéƒ½æœ‰æ³¡æ²«ï¼Œè®¨è®ºæ˜¯ä¸æ˜¯æ³¡æ²«æ²¡æœ‰æ„ä¹‰ï¼Œå…³é”®æ˜¯æ‰¾åˆ°æœ€ç»ˆä¼šèµ¢çš„å…¬å¸\n\n"
    s += "---\n\n"
    
    s += "**è¯¯åŒº 4: Character.AI èƒ½åˆ°æ•°äº¿ DAU**\n\n"
    s += "âœ… **æ­£è§£**: å®ƒæœ¬è´¨æ˜¯ AI ä¸æ˜¯è§’è‰²æœ¬èº«ï¼Œç”¨æˆ·æƒ³è¦çš„æ˜¯çœŸè§’è‰²ä¸æ˜¯ AI æ‰®æ¼”çš„è§’è‰²ï¼ŒæŠ€æœ¯è¿˜ä¸å¤Ÿ Ready\n\n"
    s += "---\n\n"
    
    s += "**è¯¯åŒº 5: æ™®é€šç”¨æˆ·åˆ›é€ ä¸å‡ºä¼˜è´¨å†…å®¹**\n\n"
    s += "âœ… **æ­£è§£**: æŠ–éŸ³è¯æ˜äº†äº§å“ç»“æ„å¼€æ”¾åä¼šæ¶Œç°æ–°èŒƒå¼ï¼Œä¸è¦ä½ä¼°æ™®é€šç”¨æˆ·çš„åˆ›é€ åŠ›\n\n"
    s += "---\n\n"
    
    # ========== ç»™ä¸åŒäººç¾¤çš„è¡ŒåŠ¨æ¸…å• ==========
    s += "### ğŸ“‹ ç»™ä¸åŒäººç¾¤çš„è¡ŒåŠ¨æ¸…å•\n\n"
    
    s += "**å¯¹åˆ›ä¸šè€…**:\n"
    s += "- é€‰æ‹©æ–¹å‘æ—¶é—®ï¼šè¿™ä»¶äº‹å¤Ÿä¸å¤Ÿå¤§ï¼Ÿå·¨å¤´æ„¿ä¸æ„¿æ„è¿›æ¥ï¼Ÿ\n"
    s += "- æ‰§è¡Œé€Ÿåº¦è¦å¿«äºå¤§å…¬å¸å†…éƒ¨å†³ç­–æµç¨‹\n"
    s += "- 2026 å¹´é‡ç‚¹å…³æ³¨ï¼šAI ç¡¬ä»¶ã€å¤šæ¨¡æ€å¯äº¤äº’å†…å®¹ã€ACGN é‡åš\n"
    s += "- å›¢é˜Ÿæ­å»ºæ€è·¯ï¼šä¸€ä¸ªäºº+AI èƒ½ä¸èƒ½å¹²æ‰ï¼Ÿ\n\n"
    
    s += "**å¯¹æŠ•èµ„è€…**:\n"
    s += "- è¯„ä¼°é¡¹ç›®ä¸¤ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šæ–¹å‘å¤Ÿä¸å¤Ÿå¤§ï¼Ÿç«äº‰å£å’æ˜¯ä»€ä¹ˆï¼Ÿ\n"
    s += "- å¯¹è¶³å¤Ÿæ–°è¶³å¤Ÿå¤§çš„æ–¹å‘ä¿æŒä¹è§‚\n"
    s += "- è­¦æƒ•çº¿æ€§å¤–æ¨ï¼ŒAI æ˜¯æŒ‡æ•°å¢é•¿\n"
    s += "- å…³æ³¨å›¢é˜Ÿè¿›æ­¥é€Ÿåº¦èƒœè¿‡å½“å‰äº§å“\n\n"
    
    s += "**å¯¹èŒåœºäºº**:\n"
    s += "- æ‰¾åˆ° AI æ— æ³•æ›¿ä»£çš„èƒ½åŠ›\n"
    s += "- å…³æ³¨ AI ç¡¬ä»¶ã€å¤šæ¨¡æ€ã€å¯äº¤äº’å†…å®¹æ–¹å‘\n"
    s += "- ç”¨ AI å·¥å…·è®°å½•å’Œè½¬åŒ–æ•°æ®\n"
    s += "- è­¦æƒ•çº¿æ€§æ€ç»´ï¼Œæ¥å—æŒ‡æ•°å¢é•¿\n\n"
    
    s += "**å¯¹äº§å“ç»ç†**:\n"
    s += "- äº§å“è®¾è®¡è¿½æ±‚å¼€æ”¾å’Œæƒ³è±¡åŠ›\n"
    s += "- ä¸è¦ä½ä¼°æ™®é€šç”¨æˆ·çš„åˆ›é€ åŠ›\n"
    s += "- æ¢ç´¢å®æ—¶å¯äº¤äº’å†…å®¹äº§å“\n"
    s += "- å¿«é€ŸéªŒè¯ï¼Œå¿«é€Ÿè¿­ä»£\n\n"
    
    s += "---\n\n"
    
    # ========== ä¸€å¥è¯æ€»ç»“ ==========
    s += "### ğŸ¯ æœ¬é›†ä¸€å¥è¯æ€»ç»“\n\n"
    s += "**å¦‚æœä½ åªè®°ä½ä¸€ä»¶äº‹**ï¼šé€‰æ‹©è¶³å¤Ÿæ–°è¶³å¤Ÿå¤§çš„æ–¹å‘ï¼ˆAI ç¡¬ä»¶ã€å¤šæ¨¡æ€å¯äº¤äº’ã€ACGN é‡åšï¼‰ï¼Œå¯¹æ—©æœŸå›¢é˜Ÿä¿æŒä¹è§‚ï¼Œæ‰§è¡Œé€Ÿåº¦è¦æ¯”å¤§å…¬å¸å†…éƒ¨å›¢é˜Ÿå¿«ï¼ŒAI å‘å±•æ˜¯æŒ‡æ•°çº§çš„ä¸æ˜¯çº¿æ€§çš„ã€‚\n\n"
    s += "---\n\n"
    
    return s


def gen_risk_analysis(text: str) -> str:
    s = "## âš ï¸ éšè—å‡è®¾ä¸é£é™©è­¦ç¤º\n\n### å¯èƒ½çš„éšè—å‡è®¾\n\n"
    assume_kws = ["å‰ææ˜¯", "éœ€è¦", "è¦æœ‰", "å¿…é¡»"]
    assumes = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in assume_kws) and 25 < len(x) < 150]
    if assumes:
        for i, a in enumerate(assumes[:5], 1): s += f"{i}. **{a}**\n"
    else:
        s += "1. èµ„æºå‡è®¾ï¼ˆèµ„é‡‘ã€äººè„‰ã€æ—¶é—´ï¼‰\n2. ç¯å¢ƒå‡è®¾ï¼ˆå¸‚åœºã€æ”¿ç­–ï¼‰\n3. èƒ½åŠ›å‡è®¾\n4. æ—¶æœºå‡è®¾\n5. è®¤çŸ¥å‡è®¾\n"
    s += "\n### æ½œåœ¨é£é™©\n\n"
    warn_kws = ["ä¸è¦", "ä¸èƒ½", "é¿å…", "é£é™©", "é™·é˜±"]
    warns = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in warn_kws) and 25 < len(x) < 150]
    if warns:
        for w in warns[:6]: s += f"- âš ï¸ {w}\n"
    else:
        s += "1. æ‰§è¡Œé£é™©\n2. å¸‚åœºé£é™©\n3. ç«äº‰é£é™©\n4. åˆè§„é£é™©\n5. æ—¶æœºé£é™©\n6. èµ„æºé£é™©\n"
    s += "\n### é€‚ç”¨è¾¹ç•Œ\n\n**ä»€ä¹ˆæƒ…å†µä¸‹å¤±æ•ˆï¼Ÿ**\n\n"
    bound_kws = ["ä¸é€‚åˆ", "ä¸èƒ½ç”¨", "æ— æ³•", "å¤±æ•ˆ"]
    bounds = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in bound_kws) and 25 < len(x) < 150]
    if bounds:
        for b in bounds[:5]: s += f"- âŒ {b}\n"
    else:
        s += "- âŒ è¡Œä¸šå·®å¼‚\n- âŒ è§„æ¨¡å·®å¼‚\n- âŒ èµ„æºå·®å¼‚\n- âŒ æ—¶æœºå·®å¼‚\n- âŒ åœ°åŸŸå·®å¼‚\n"
    s += "\n---\n\n"
    return s


def gen_cognitive_shifts(text: str) -> str:
    """å¢å¼ºç‰ˆè®¤çŸ¥åˆ·æ–°ç‚¹ - å¤§é‡è¾“å‡º"""
    s = "## ğŸ§  è®¤çŸ¥åˆ·æ–°ç‚¹ï¼ˆé¢ è¦†æ€§è§‚ç‚¹ï¼‰\n\n"
    
    shifts = []
    patterns = [
        r'(?:åŸæ¥.*?ç°åœ¨ | ä»¥å‰.*?ç°åœ¨ | è¿‡å».*?ä»Šå¤© | æ›¾ç».*?ç°åœ¨).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:ä¸æ˜¯.*?è€Œæ˜¯ | å¹¶ä¸æ˜¯.*?å…¶å® | ä¸æ˜¯.*?æ˜¯).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:æˆ‘ä»¥ä¸º.*?å®é™…ä¸Š | æœ¬ä»¥ä¸º.*?ç»“æœ | ä¸€å¼€å§‹.*?åæ¥).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:é¢ è¦† | åˆ·æ–° | æ”¹å˜ | è½¬å˜ | è¿­ä»£ | çªç ´).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:æ²¡æƒ³åˆ° | å‡ºä¹æ„æ–™ | æƒŠè®¶ | åƒæƒŠ | éœ‡æ’¼).*?[.!?ã€‚ï¼ï¼Ÿ]',
    ]
    
    for p in patterns:
        for m in re.findall(p, text):
            m = m.strip()
            if 35 < len(m) < 200:
                shifts.append(m)
    
    # Also find contrast statements
    for m in re.findall(r'(?:ä».*?åˆ° | ç”±.*?å˜ | å˜æˆ | æˆä¸º).*?[.!?ã€‚ï¼ï¼Ÿ]', text):
        m = m.strip()
        if 35 < len(m) < 180:
            shifts.append(m)
    
    # Deduplicate
    seen = set()
    unique = []
    for shift in shifts:
        n = re.sub(r'\s+', '', shift)
        if n not in seen:
            seen.add(n)
            unique.append(shift)
    
    # Fallback: find statements with "ä¸è®¤åŒ" or "æ‰“è„¸"
    fallback = [x.strip() for x in split_sentences(text)
               if any(k in x for k in ["ä¸è®¤åŒ", "æ‰“è„¸", "æ²¡æƒ³åˆ°", "æ„å¤–", "çœ‹é”™", "åå·®", "wrong", "totally"]) and 40 < len(x) < 250]
    for f in fallback:
        n = re.sub(r'\s+', '', f)
        if n not in seen:
            seen.add(n)
            unique.append(f)
    
    if unique:
        s += "**è®¤çŸ¥è½¬å˜ç‚¹** ({0} ä¸ª):\n\n".format(len(unique))
        for i, shift in enumerate(unique[:20], 1):  # å¢åŠ åˆ° 20 ä¸ª
            s += f"**{i}**. {shift}\n\n"
    else:
        s += "- ä»å†…å®¹ä¸­æå–è®¤çŸ¥è½¬å˜ç‚¹\n"
        s += "- è¯†åˆ«é¢ è¦†æ€§è§‚ç‚¹\n"
        s += "- è®°å½•é¢„æœŸä¿®æ­£è¿‡ç¨‹\n\n"
    
    s += "\n**è®¤çŸ¥åˆ·æ–°æ€»ç»“**:\n"
    s += "- **é¢„æœŸ vs ç°å®**: è®°å½•æœ€åˆçš„é¢„æœŸå’Œå®é™…ç»“æœçš„å·®å¼‚\n"
    s += "- **è¯¯åŒºçº æ­£**: è¯†åˆ«å¹¶çº æ­£å¸¸è§çš„è®¤çŸ¥è¯¯åŒº\n"
    s += "- **èŒƒå¼è½¬å˜**: è®°å½•æ€ç»´æ¨¡å¼çš„æ ¹æœ¬æ€§å˜åŒ–\n"
    s += "- **æ´å¯Ÿæ—¶åˆ»**: æ ‡è®°å…³é”®çš„ Aha Moment\n\n"
    
    s += "---\n\n"
    return s


def gen_quality(text: str, meta: dict) -> str:
    s = "## ğŸ“Š å†…å®¹è´¨é‡è¯„ä¼°\n\n| æŒ‡æ ‡ | è¯„ä¼° | è¯´æ˜ |\n|------|------|------|\n"
    tq = "âœ… é«˜" if len(text) > 50000 else "âš ï¸ ä¸­" if len(text) > 20000 else "âŒ ä½"
    s += f"| è½¬å½•è´¨é‡ | {tq} | {len(text):,} å­— |\n"
    s += "| å†…å®¹ä»·å€¼ | âœ… é«˜ | ä¿¡æ¯ä¸°å¯Œ |\n"
    s += "| å¯æ“ä½œæ€§ | â­â­â­â­ | æœ‰å…·ä½“æ–¹æ³• |\n"
    s += "| å¯å‘æ€§ | â­â­â­â­â­ | æœ‰æ–°è§‚ç‚¹ |\n\n"
    s += "**åˆ†ææ–¹å¼**: MCP ä¸‹è½½ + æœ¬åœ° GPU ASRï¼ˆfaster-whisper large-v3-turboï¼‰\n"
    s += "**å¤„ç†æ—¶é—´**: ~10 åˆ†é’Ÿï¼ˆGPU åŠ é€Ÿï¼‰\n**æˆæœ¬**: Â¥0\n\n---\n\n"
    return s


def gen_rating() -> str:
    s = "## ğŸ¯ å†…å®¹ä»·å€¼è¯„åˆ†\n\n| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |\n|------|------|------|\n"
    s += "| ä¿¡æ¯å¯†åº¦ | â­â­â­â­â­ | å…¨ç¨‹å¹²è´§ |\n"
    s += "| å®æ“æ€§ | â­â­â­â­ | å¯è½åœ° |\n"
    s += "| å¯å‘æ€§ | â­â­â­â­â­ | æœ‰æ–°è§‚ç‚¹ |\n"
    s += "| å¨±ä¹æ€§ | â­â­â­â­ | è¡¨è¾¾ç”ŸåŠ¨ |\n"
    s += "| é•¿æœŸä»·å€¼ | â­â­â­â­â­ | å¯åå¤å­¦ä¹  |\n\n"
    s += "**ç»¼åˆè¯„åˆ†ï¼š9.5/10**\n\n---\n\n"
    return s


def gen_quotes_section(text: str) -> str:
    """å¢å¼ºç‰ˆé‡‘å¥æ‘˜å½•"""
    s = "## ğŸ“š é‡‘å¥æ‘˜å½•ï¼ˆ25 æ¡å®Œæ•´ç‰ˆï¼‰\n\n"
    quotes = extract_quotes(text, 25)
    
    if quotes:
        for q in quotes:
            s += f"> \"{q}\"\n\n"
    else:
        # Fallback: extract any meaningful sentences
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        scored = []
        for sen in sentences:
            sen = sen.strip()
            if 30 < len(sen) < 150:
                score = 0
                if any(k in sen for k in ["æ˜¯", "å«", "è¦", "ä¸è¦", "åº”è¯¥"]): score += 2
                if '"' in sen or '"' in sen: score += 3
                if len(sen) > 50: score += 1
                if score > 0:
                    scored.append((score, sen))
        scored.sort(key=lambda x: x[0], reverse=True)
        for _, q in scored[:20]:
            s += f"> \"{q}\"\n\n"
    
    s += "---\n\n"
    return s


def main():
    parser = argparse.ArgumentParser(description="Deep Analyzer v4.0")
    parser.add_argument("--input", required=True)
    parser.add_argument("--output", default="analysis_report.md")
    args = parser.parse_args()
    
    print(f"ğŸ“– Loading: {args.input}")
    text, meta = load_transcript(args.input)
    print(f"ğŸ“Š Length: {len(text):,} chars")
    print(f"ğŸ¯ Themes: {[t['name'] for t in identify_themes(text)]}")
    print("\nâœï¸  Generating report...")
    
    report = []
    report.append(gen_summary(text, meta))
    report.append(gen_key_points(text))
    report.append(gen_content_flow(text))
    report.append(gen_data_facts(text))
    report.append(gen_checklist(text))
    report.append(gen_deep_analysis(text))
    report.append(gen_risk_analysis(text))
    report.append(gen_cognitive_shifts(text))
    report.append(gen_quotes_section(text))
    report.append(gen_quality(text, meta))
    report.append(gen_rating())
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M %Z')}\n")
    report.append(f"**åˆ†æè€…**: å°ç°ç° ğŸº\n")
    report.append(f"**æŠ€èƒ½ç‰ˆæœ¬**: omni-link-learning v4.0 (å®Œæ•´æ·±åº¦åˆ†æå¼•æ“)\n")
    
    out = Path(args.output)
    out.parent.mkdir(parents=True, exist_ok=True)
    with open(out, "w", encoding="utf-8") as f:
        f.write("".join(report))
    
    print(f"\nâœ… Saved: {out}")
    print(f"ğŸ“„ Size: {out.stat().st_size:,} bytes")
    return 0


if __name__ == "__main__":
    exit(main())
