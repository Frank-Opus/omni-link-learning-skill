#!/usr/bin/env python3
"""
Deep Analyzer v4.0 - å®Œæ•´æ·±åº¦åˆ†æå¼•æ“
æ ¸å¿ƒç†å¿µï¼šä¸é—æ¼ä»»ä½•æœ‰ä»·å€¼çš„å†…å®¹

v4.0 æ–°å¢:
- å…³é”®è¦ç‚¹ 8â†’15 ä¸ª
- é‡‘å¥ 15â†’25 ä¸ª
- æ–°å¢"å®Œæ•´å†…å®¹è„‰ç»œ"ç« èŠ‚
- æ–°å¢"å…³é”®æ•°æ®ä¸äº‹å®æå–"
- æ–°å¢"å®æˆ˜åº”ç”¨æ¸…å•"
- æ–°å¢"è®¤çŸ¥åˆ·æ–°ç‚¹"
- å¢å¼ºæ™ºèƒ½å¡«å……
"""

import argparse
import json
import re
from pathlib import Path
from datetime import datetime


def load_transcript(input_path: str) -> tuple[str, dict]:
    """Load transcript and metadata."""
    path = Path(input_path)
    meta_path = path.parent / "douyin_mcp_result.json"
    metadata = {}
    
    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
            video_info = meta.get("video_info", {})
            if isinstance(video_info, dict):
                metadata["title"] = video_info.get("title", "Unknown")
                metadata["author"] = video_info.get("author", "Unknown")
                metadata["platform"] = meta.get("platform", "Unknown")
            
            if "transcript" in meta and meta["transcript"]:
                try:
                    data = json.loads(meta["transcript"])
                    if isinstance(data, dict):
                        if "text" in data:
                            return data["text"].strip(), metadata
                        elif "segments" in data:
                            text = "".join([s.get("text", "") for s in data["segments"]])
                            return text.strip(), metadata
                except: pass
    
    with open(path, "r", encoding="utf-8") as f:
        content = f.read().strip()
    
    if content.startswith('{'):
        try:
            data = json.loads(content)
            if isinstance(data, dict):
                if "text" in data:
                    return data["text"].strip(), metadata
                elif "segments" in data:
                    text = "".join([s.get("text", "") for s in data["segments"]])
                    return text.strip(), metadata
        except: pass
    
    return content, metadata


def identify_themes(text: str) -> list:
    themes = {
        "èŒåœºæˆé•¿": ["åŠªåŠ›", "è§„åˆ’", "æœºä¼š", "è·³æ§½", "æ·±è€•", "é•¿æœŸä¸»ä¹‰"],
        "é”€å”®æŠ€å·§": ["æ‹œè®¿", "å®¢æˆ·", "ä¿¡ä»»", "æˆäº¤", "é™Œæ‹œ", "ä¸šç»©"],
        "è‡ªåª’ä½“": ["æµé‡", "ç²‰ä¸", "è§†é¢‘", "çˆ†æ¬¾", "ç®—æ³•", "è·å®¢"],
        "AI ä¸æŠ€æœ¯": ["AI", "å·¥å…·", "è‡ªåŠ¨åŒ–", "æ¨¡å‹", "Agent"],
        "æŠ•èµ„æ€ç»´": ["æŠ•èµ„", "å‘¨æœŸ", "éå…±è¯†", "åˆ›å§‹äºº", "æœºä¼š"],
        "å•†ä¸šæ´å¯Ÿ": ["å¸‚åœº", "ç«äº‰", "åˆ©æ¶¦", "æ¨¡å¼", "ç”Ÿæ€"],
    }
    result = []
    for name, kws in themes.items():
        count = sum(text.count(k) for k in kws)
        if count > 3:
            result.append({"name": name, "count": count})
    result.sort(key=lambda x: x["count"], reverse=True)
    return result[:6]


def split_sentences(text: str) -> list:
    """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ - å¤„ç†æ— æ ‡ç‚¹ ASR è½¬å½•"""
    # First try normal sentence splitting
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
    
    # If too few sentences, try semantic splitting
    if len(sentences) < 10:
        # Split by common connectors
        connectors = [' å› ä¸º ', ' æ‰€ä»¥ ', ' ä½†æ˜¯ ', ' ç„¶å ', ' å°± ', ' é‚£ ', ' å¯¹ ', ' å…¶å® ', ' æˆ‘è§‰å¾— ', ' æˆ‘è®¤ä¸º ']
        result = [text]
        for conn in connectors:
            new_result = []
            for s in result:
                if len(s) > 300:  # Only split long segments
                    parts = s.split(conn)
                    for i, p in enumerate(parts):
                        if i > 0 and len(p) > 20:
                            new_result.append(conn.strip() + p)
                        elif len(p) > 20:
                            new_result.append(p)
                else:
                    new_result.append(s)
            result = new_result
        
        # Split by length (max 200 chars)
        final = []
        for s in result:
            if len(s) > 200:
                # Split at natural pauses
                for i in range(0, len(s), 150):
                    chunk = s[i:i+150]
                    if len(chunk) > 30:
                        final.append(chunk)
            elif len(s) > 30:
                final.append(s)
        return final
    
    # Filter and clean
    cleaned = []
    for s in sentences:
        s = s.strip()
        if len(s) > 20:
            cleaned.append(s)
    return cleaned


def extract_quotes(text: str, max_q: int = 25) -> list:
    """å¢å¼ºç‰ˆé‡‘å¥æå– - å¤„ç† ASR è½¬å½•"""
    quotes = []
    sentences = split_sentences(text)
    
    # Pattern 1: Direct quotes
    for s in sentences:
        if '"' in s or '"' in s:
            matches = re.findall(r'[""](.*?)[""]', s)
            for m in matches:
                if 20 < len(m) < 150:
                    quotes.append(m.strip())
    
    # Pattern 2: Importance markers
    markers = ["æœ€é‡è¦çš„æ˜¯", "å…³é”®æ˜¯", "æ ¸å¿ƒ", "è®°ä½", "ä¸€å®šè¦", "æœ¬è´¨ä¸Š", "æˆ‘è®¤ä¸º", 
               "æˆ‘è§‰å¾—", "æˆ‘å°è±¡", "æˆ‘å‘ç°", "æˆ‘çš„è§‚ç‚¹", "è¯´ç™½äº†", "å¬å¥½", "æˆ‘è·Ÿä½ è®²",
               "æˆ‘çš„æ„Ÿå—", "æˆ‘è‡ªå·±", "åœ¨æˆ‘çœ‹æ¥"]
    for s in sentences:
        if any(m in s for m in markers):
            if 30 < len(s) < 200:
                quotes.append(s)
    
    # Pattern 3: Contrast patterns
    for s in sentences:
        if ("ä¸æ˜¯" in s and "è€Œæ˜¯" in s) or ("ä»" in s and "åˆ°" in s and len(s) > 40):
            if 40 < len(s) < 200:
                quotes.append(s)
    
    # Pattern 4: Definition patterns  
    for s in sentences:
        if any(k in s for k in ["å«åš", "å°±æ˜¯", "ç­‰äº", "æ„å‘³ç€", "æ˜¯ç¬¬ä¸€ä¸ª"]):
            if 30 < len(s) < 180:
                quotes.append(s)
    
    # Pattern 5: Advice patterns
    for s in sentences:
        if any(k in s for k in ["è¦ ", "ä¸è¦ ", "åº”è¯¥ ", "å¿…é¡» ", "å¯ä»¥ "]):
            if 25 < len(s) < 180 and len(s.split(' ')) < 30:
                quotes.append(s)
    
    # Pattern 6: Insight patterns
    for s in sentences:
        if any(k in s for k in ["éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "æ„å¤–", "é¢ è¦†", "åˆ·æ–°", "çªç ´"]):
            if 30 < len(s) < 180:
                quotes.append(s)
    
    # Deduplicate
    seen = set()
    unique = []
    for q in quotes:
        n = re.sub(r'\s+', '', q)
        if n not in seen and len(q) > 25:
            seen.add(n)
            unique.append(q)
    
    # Sort by length and quality
    unique.sort(key=lambda x: (-len(x), x))
    return unique[:max_q]


def extract_data(text: str) -> list:
    data = []
    for m in re.findall(r'(\d+(?:\.\d+)?(?:ä¸‡ | äº¿ | å€ | å¹´ | ä¸ªæœˆ |%|ï¼…))', text):
        idx = text.find(m)
        ctx = text[max(0,idx-40):min(len(text),idx+len(m)+40)].strip()
        data.append({"type": "æ•°æ®", "value": m, "context": ctx})
    for m in re.findall(r'(å­—èŠ‚ | æŠ–éŸ³ | è…¾è®¯ | é˜¿é‡Œ | ç¾å›¢ | å°çº¢ä¹¦ |Google|Meta|OpenAI|Midjourney)', text):
        data.append({"type": "æ¡ˆä¾‹", "value": m, "context": ""})
    seen = set()
    unique = []
    for d in data:
        k = (d["type"], d["value"])
        if k not in seen:
            seen.add(k)
            unique.append(d)
    return unique[:25]


def extract_advice(text: str) -> list:
    advice = []
    for p in [r'è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'ä¸è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'åº”è¯¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'å¿…é¡» (.*?)[.!?ã€‚ï¼ï¼Ÿ]']:
        for m in re.findall(p, text):
            m = m.strip()
            if 15 < len(m) < 200:
                advice.append(m)
    seen = set()
    unique = []
    for a in advice:
        if a[:50] not in seen:
            seen.add(a[:50])
            unique.append(a)
    return unique[:20]


def gen_summary(text: str, meta: dict) -> str:
    title = meta.get("title", "æœªå‘½å")
    platform = meta.get("platform", "Unknown")
    themes = identify_themes(text)
    theme_str = "ã€".join([t["name"] for t in themes]) if themes else "ç»¼åˆå†…å®¹"
    quotes = extract_quotes(text, 4)
    
    s = f"# ğŸ“Š å®Œæ•´åˆ†ææŠ¥å‘Š\n\n## ğŸ“‹ è§†é¢‘å…ƒæ•°æ®\n\n"
    s += f"- **æ¥æº**: {platform}\n- **æ ‡é¢˜**: {title}\n- **è½¬å½•é•¿åº¦**: {len(text):,} å­—\n"
    s += f"- **è§†é¢‘æ—¶é•¿**: çº¦ {len(text)//250} åˆ†é’Ÿ\n- **åˆ†ææ–¹æ³•**: MCP ä¸‹è½½ + æœ¬åœ° GPU ASR\n\n---\n\n"
    s += f"## ğŸ¯ æ ¸å¿ƒæ‘˜è¦ï¼ˆ30 ç§’é€Ÿè¯»ï¼‰\n\næœ¬è§†é¢‘æ ¸å¿ƒä¸»é¢˜ï¼š**{theme_str}**\n\n"
    s += "**æ ¸å¿ƒè§‚ç‚¹**:\n"
    for q in quotes:
        s += f"> \"{q}\"\n\n"
    s += "**ä¸ºä»€ä¹ˆå€¼å¾—çœ‹**:\n- âœ… å®æˆ˜ç»éªŒï¼Œéç†è®ºç©ºè°ˆ\n- âœ… å…·ä½“æ–¹æ³•è®º\n- âœ… æœ‰æ¡ˆä¾‹æ”¯æ’‘\n- âœ… æœ‰æ•°æ®éªŒè¯\n\n---\n\n"
    return s


def gen_key_points(text: str) -> str:
    """å¢å¼ºç‰ˆå…³é”®è¦ç‚¹æå– - å¤„ç† ASR è½¬å½•"""
    s = "## ğŸ“ å…³é”®è¦ç‚¹æ·±åº¦è§£è¯»ï¼ˆ15 ä¸ªå®Œæ•´ç‰ˆï¼‰\n\n"
    sentences = split_sentences(text)
    kws = ["æœ€é‡è¦çš„æ˜¯", "å…³é”®æ˜¯", "æ ¸å¿ƒ", "è®°ä½", "ä¸€å®šè¦", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰", "æ€»ç»“", 
           "æœ¬è´¨ä¸Š", "æˆ‘è®¤ä¸º", "æˆ‘è§‰å¾—", "å…¬å¼", "æ³•åˆ™", "æ­¥éª¤", "æ–¹æ³•", "å¬å¥½", "è¯´ç™½äº†",
           "éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "é¢ è¦†", "åˆ·æ–°", "çªç ´", "æˆ‘çš„æ„Ÿå—", "åœ¨æˆ‘çœ‹æ¥"]
    
    scored = []
    for i, sen in enumerate(sentences):
        sen = sen.strip()
        if len(sen) < 30 or len(sen) > 300: continue
        score = sum(3 for k in kws if k in sen)
        if re.search(r'\d+', sen): score += 2
        if any(w in sen for w in ["è¦ ", "ä¸è¦ ", "åº”è¯¥ ", "å¿…é¡» "]): score += 2
        if '"' in sen or '"' in sen: score += 3
        if "ä¸æ˜¯" in sen and "è€Œæ˜¯" in sen: score += 3
        if "ä»" in sen and "åˆ°" in sen: score += 2
        if any(k in sen for k in ["éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "é¢ è¦†"]): score += 3
        if score > 0:
            scored.append((score, sen, i))
    
    scored.sort(key=lambda x: x[0], reverse=True)
    
    for i, (score, point, idx) in enumerate(scored[:15], 1):
        point = point.strip()
        if point.startswith(('ï¼Œ', 'ã€‚', 'ã€', ' ')):
            point = point.lstrip('ï¼Œã€‚ã€ ')
        
        s += f"### {i}. {point}\n\n"
        s += "**æ·±åº¦è§£è¯»**:\n"
        
        # Find context
        ctx_start = max(0, idx - 2)
        ctx_end = min(len(sentences), idx + 3)
        context = " ".join([sentences[j].strip() for j in range(ctx_start, ctx_end) if len(sentences[j].strip()) > 20])
        
        if any(k in point for k in ["æ–¹æ³•", "æ­¥éª¤", "æ€ä¹ˆ", "å¦‚ä½•", "å…¬å¼", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰"]):
            s += "- ğŸ”§ **æ–¹æ³•è®º**: è¿™æ˜¯ä¸€ä¸ªå…·ä½“çš„æ“ä½œæ–¹æ³•\n"
            if context: s += f"- ğŸ“– **ä¸Šä¸‹æ–‡**: {context[:150]}...\n"
            s += "- âœ… **æ‰§è¡Œè¦ç‚¹**: æ³¨æ„å…³é”®æ‰§è¡Œç»†èŠ‚\n\n"
        elif any(k in point for k in ["ä¸è¦", "é¿å…", "é£é™©", "ä¸èƒ½", "åƒä¸‡åˆ«", "æ— æ³•"]):
            s += "- âš ï¸ **è­¦ç¤º**: è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ³¨æ„çš„é£é™©ç‚¹\n"
            s += "- ğŸ” **é£é™©æ¥æº**: è¯†åˆ«é£é™©çš„æ ¹æº\n"
            s += "- ğŸ›¡ï¸ **è§„é¿æ–¹æ³•**: å¦‚ä½•é¿å…è¿™ä¸ªé£é™©\n\n"
        elif any(k in point for k in ["è¦ ", "åº”è¯¥ ", "å¿…é¡» ", "ä¸€å®š"]):
            s += "- âœ… **è¡ŒåŠ¨æŒ‡å—**: è¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„è¡ŒåŠ¨å»ºè®®\n"
            for j in range(idx, min(len(sentences), idx + 3)):
                if any(p in sentences[j] for p in ["å› ä¸º", "æ‰€ä»¥", "å¦åˆ™", "ä¸ç„¶", "æ‰èƒ½"]):
                    s += f"- ğŸ’¡ **åŸå› **: {sentences[j].strip()[:120]}...\n"
                    break
            s += "- ğŸ“‹ **å¦‚ä½•æ‰§è¡Œ**: æ‹†è§£ä¸ºå…·ä½“æ­¥éª¤\n\n"
        elif "ä¸æ˜¯" in point and "è€Œæ˜¯" in point:
            s += "- ğŸ”„ **å¯¹æ¯”/çº æ­£**: è¿™æ˜¯ä¸€ä¸ªè®¤çŸ¥çº æ­£\n"
            s += "- âŒ **å¸¸è§è¯¯åŒº**: äººä»¬é€šå¸¸æ€ä¹ˆæƒ³\n"
            s += "- âœ… **æ­£ç¡®ç†è§£**: å®é™…åº”è¯¥æ€ä¹ˆçœ‹\n\n"
        elif any(k in point for k in ["å«åš", "å°±æ˜¯", "ç­‰äº", "æ„å‘³ç€", "æ˜¯ç¬¬ä¸€ä¸ª"]):
            s += "- ğŸ’ **å®šä¹‰/æ´å¯Ÿ**: è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µæˆ–æ´å¯Ÿ\n"
            if context: s += f"- ğŸ“– **èƒŒæ™¯**: {context[:120]}...\n"
            s += "- ğŸ¯ **åº”ç”¨**: å¦‚ä½•åº”ç”¨åˆ°ä½ çš„æƒ…å†µ\n\n"
        else:
            s += "- ğŸ’¡ **è§‚ç‚¹**: è¿™æ˜¯ä¸€ä¸ªæ´å¯Ÿæˆ–è§‚ç‚¹\n"
            if context: s += f"- ğŸ“– **èƒŒæ™¯**: {context[:120]}...\n"
            s += "- ğŸ¯ **åº”ç”¨**: å¦‚ä½•åº”ç”¨åˆ°ä½ çš„æƒ…å†µ\n\n"
        
        s += "---\n\n"
    return s


def gen_content_flow(text: str) -> str:
    s = "## ğŸ“– å®Œæ•´å†…å®¹è„‰ç»œï¼ˆæŒ‰é€»è¾‘é¡ºåºï¼‰\n\n"
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
    chunk_size = 15
    chunks = []
    for i in range(0, len(sentences), chunk_size):
        chunk = " ".join([s.strip() for s in sentences[i:i+chunk_size] if len(s.strip()) > 10])
        if len(chunk) > 50:
            chunks.append(chunk[:400])
    
    for i, chunk in enumerate(chunks[:8], 1):
        s += f"**ç¬¬{i}éƒ¨åˆ†**: {chunk}...\n\n"
    s += "---\n\n"
    return s


def gen_data_facts(text: str) -> str:
    s = "## ğŸ“Š å…³é”®æ•°æ®ä¸äº‹å®æå–\n\n"
    data = extract_data(text)
    by_type = {}
    for d in data:
        t = d["type"]
        if t not in by_type: by_type[t] = []
        by_type[t].append(d)
    
    for t, items in by_type.items():
        s += f"**{t}**:\n"
        for item in items[:8]:
            if item["context"]:
                s += f"- `{item['value']}` â€” {item['context'][:80]}...\n"
            else:
                s += f"- `{item['value']}`\n"
        s += "\n"
    s += "---\n\n"
    return s


def gen_checklist(text: str) -> str:
    """å¢å¼ºç‰ˆå®æˆ˜æ¸…å•"""
    s = "## âœ… å®æˆ˜åº”ç”¨æ¸…å•ï¼ˆå¯ç›´æ¥æ‰§è¡Œï¼‰\n\n"
    
    advice = []
    # Extract actionable advice
    patterns = [
        r'è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ä¸è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', 
        r'åº”è¯¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'å¿…é¡» (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'å¯ä»¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ç¬¬ä¸€æ­¥ [ï¼Œ,]*(.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'é¦–å…ˆ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ç„¶å (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'æœ€å (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
    ]
    
    for p in patterns:
        for m in re.findall(p, text):
            m = m.strip()
            if 15 < len(m) < 200:
                advice.append(m)
    
    # Deduplicate
    seen = set()
    unique = []
    for a in advice:
        if a[:50] not in seen:
            seen.add(a[:50])
            unique.append(a)
    
    if unique:
        for a in unique[:20]:
            s += f"- [ ] {a}\n"
    else:
        s += "- ä»å†…å®¹ä¸­æå–å¯æ‰§è¡Œå»ºè®®\n"
        s += "- æ•´ç†ä¸ºè¡ŒåŠ¨æ¸…å•\n"
    
    s += "\n---\n\n"
    return s


def gen_deep_analysis(text: str) -> str:
    """å¢å¼ºç‰ˆæ·±åº¦åˆ†æ - ä»åŸæ–‡æ™ºèƒ½æå–"""
    s = "## ğŸ’¡ æ·±åº¦åˆ†æä¸æ´å¯Ÿ\n\n### åº•å±‚é€»è¾‘åˆ†æ\n\n**ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ**\n\n"
    
    sentences = split_sentences(text)
    
    # Human nature insights
    human_kws = ["äºº", "äººæ€§", "å¿ƒç†", "éœ€è¦", "æƒ³è¦", "å®³æ€•", "è§‰å¾—", "æ„Ÿè§‰", "å¸Œæœ›", "æ¸´æœ›", "å–œæ¬¢", "è®¨åŒ"]
    human = [x for x in sentences if any(k in x for k in human_kws) and 40 < len(x) < 250]
    s += "1. **äººæ€§å±‚é¢**:\n"
    if human:
        for h in human[:5]:
            s += f"   - {h}\n"
    else:
        s += "   - æ»¡è¶³åŸºæœ¬éœ€æ±‚ï¼šè¢«çœ‹è§ã€è¢«è®¤å¯\n"
    s += "\n"
    
    # Business logic
    biz_kws = ["ä»·å€¼", "åˆ©ç›Š", "æˆæœ¬", "æ”¶ç›Š", "æ•ˆç‡", "åˆ©æ¶¦", "èµšé’±", "ç”Ÿæ„", "èµ„æº", "äº¤æ¢", "ä¹°å–", "å•†ä¸š"]
    biz = [x for x in sentences if any(k in x for k in biz_kws) and 40 < len(x) < 250]
    s += "2. **å•†ä¸š/ä»·å€¼å±‚é¢**:\n"
    if biz:
        for b in biz[:5]:
            s += f"   - {b}\n"
    else:
        s += "   - åˆ›é€ ä»·å€¼ï¼Œè§£å†³ç—›ç‚¹\n"
    s += "\n"
    
    # System thinking
    sys_kws = ["ç³»ç»Ÿ", "å¾ªç¯", "æ æ†", "è§„æ¨¡", "æ¨¡å¼", "ç”Ÿæ€", "å¹³å°", "ç½‘ç»œ", "å¤åˆ¶", "æ”¾å¤§", "å¤åˆ©"]
    sys = [x for x in sentences if any(k in x for k in sys_kws) and 40 < len(x) < 250]
    s += "3. **ç³»ç»Ÿ/æ¨¡å¼å±‚é¢**:\n"
    if sys:
        for x in sys[:5]:
            s += f"   - {x}\n"
    else:
        s += "   - ç³»ç»Ÿæ€§æ æ†æˆ–å¯å¤åˆ¶æ¨¡å¼\n"
    s += "\n"
    
    # Pattern recognition
    s += "### æ¨¡å¼è¯†åˆ«\n\n**åæ˜ çš„æ›´å¤§æ¨¡å¼ï¼Ÿ**\n\n"
    formulas = []
    for p in [r'å«åš (?:ä¸€ä¸ª)?([ çš„ A-Za-z0-9+\-]+(?:å…¬å¼ | æ³•åˆ™ | æ–¹æ³• | ç­–ç•¥ | æ¨¡å¼))',
              r'(?:ç¬¬ä¸€ | ç¬¬äºŒ | ç¬¬ä¸‰ | ç¬¬å›› | ç¬¬äº”)[ã€.]\s*(.+?)[.!?ã€‚ï¼ï¼Ÿ]']:
        for m in re.findall(p, text):
            m = m.strip() if isinstance(m, str) else m[0].strip()
            if 15 < len(m) < 150:
                formulas.append(m)
    
    if formulas:
        s += "**æå–çš„å…¬å¼/æ¨¡å¼**:\n"
        for f in formulas[:6]:
            s += f"- {f}\n"
        s += "\n"
    
    s += "- **å¯å¤ç”¨å…¬å¼**: ä»å†…å®¹æç‚¼æ ¸å¿ƒæ–¹æ³•è®º\n"
    s += "- **å…³é”®å˜é‡**: å½±å“ç»“æœçš„æ ¸å¿ƒå› ç´ \n\n"
    
    # Cross-case comparison
    s += "### è·¨æ¡ˆä¾‹å¯¹æ¯”\n\n**ä¸å…¶ä»–æ¡ˆä¾‹çš„å¼‚åŒï¼Ÿ**\n\n"
    
    unique = [x for x in sentences if any(k in x for k in ["ä¸æ˜¯", "è€Œä¸æ˜¯", "ä¸åŒäº", "å…³é”®æ˜¯", "æ ¸å¿ƒ", "ç›¸æ¯”"]) and 50 < len(x) < 250]
    
    s += "| ç»´åº¦ | æœ¬æ¡ˆä¾‹ | å…¸å‹åšæ³• | å·®å¼‚åˆ†æ |\n|------|--------|----------|----------|\n"
    if unique:
        s += f"| æ–¹æ³• | ä»å†…å®¹æå– | å¸¸è§„åšæ³• | {unique[0][:50]}... |\n"
        if len(unique) > 1:
            s += f"| æ•ˆæœ | æ›´ç²¾å‡†æœ‰æ•ˆ | æ•ˆæœä¸€èˆ¬ | {unique[1][:50]}... |\n"
        else:
            s += "| æ•ˆæœ | æ›´ç²¾å‡†æœ‰æ•ˆ | æ•ˆæœä¸€èˆ¬ | é’ˆå¯¹æ€§æ›´å¼º |\n"
        s += "| é€‚ç”¨ | ç‰¹å®šåœºæ™¯ | é€šç”¨åœºæ™¯ | éœ€åˆ¤æ–­è¾¹ç•Œ |\n"
    else:
        s += "| æ–¹æ³• | å¾…åˆ†æ | å¸¸è§„åšæ³• | å¾…å¯¹æ¯” |\n"
        s += "| æ•ˆæœ | å¾…è¯„ä¼° | ä¸€èˆ¬æ•ˆæœ | å¾…åˆ†æ |\n"
        s += "| é€‚ç”¨ | å¾…è¯†åˆ« | é€šç”¨åœºæ™¯ | å¾…æ˜ç¡® |\n"
    s += "\n---\n\n"
    return s


def gen_risk_analysis(text: str) -> str:
    s = "## âš ï¸ éšè—å‡è®¾ä¸é£é™©è­¦ç¤º\n\n### å¯èƒ½çš„éšè—å‡è®¾\n\n"
    assume_kws = ["å‰ææ˜¯", "éœ€è¦", "è¦æœ‰", "å¿…é¡»"]
    assumes = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in assume_kws) and 25 < len(x) < 150]
    if assumes:
        for i, a in enumerate(assumes[:5], 1): s += f"{i}. **{a}**\n"
    else:
        s += "1. èµ„æºå‡è®¾ï¼ˆèµ„é‡‘ã€äººè„‰ã€æ—¶é—´ï¼‰\n2. ç¯å¢ƒå‡è®¾ï¼ˆå¸‚åœºã€æ”¿ç­–ï¼‰\n3. èƒ½åŠ›å‡è®¾\n4. æ—¶æœºå‡è®¾\n5. è®¤çŸ¥å‡è®¾\n"
    s += "\n### æ½œåœ¨é£é™©\n\n"
    warn_kws = ["ä¸è¦", "ä¸èƒ½", "é¿å…", "é£é™©", "é™·é˜±"]
    warns = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in warn_kws) and 25 < len(x) < 150]
    if warns:
        for w in warns[:6]: s += f"- âš ï¸ {w}\n"
    else:
        s += "1. æ‰§è¡Œé£é™©\n2. å¸‚åœºé£é™©\n3. ç«äº‰é£é™©\n4. åˆè§„é£é™©\n5. æ—¶æœºé£é™©\n6. èµ„æºé£é™©\n"
    s += "\n### é€‚ç”¨è¾¹ç•Œ\n\n**ä»€ä¹ˆæƒ…å†µä¸‹å¤±æ•ˆï¼Ÿ**\n\n"
    bound_kws = ["ä¸é€‚åˆ", "ä¸èƒ½ç”¨", "æ— æ³•", "å¤±æ•ˆ"]
    bounds = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in bound_kws) and 25 < len(x) < 150]
    if bounds:
        for b in bounds[:5]: s += f"- âŒ {b}\n"
    else:
        s += "- âŒ è¡Œä¸šå·®å¼‚\n- âŒ è§„æ¨¡å·®å¼‚\n- âŒ èµ„æºå·®å¼‚\n- âŒ æ—¶æœºå·®å¼‚\n- âŒ åœ°åŸŸå·®å¼‚\n"
    s += "\n---\n\n"
    return s


def gen_cognitive_shifts(text: str) -> str:
    """å¢å¼ºç‰ˆè®¤çŸ¥åˆ·æ–°ç‚¹"""
    s = "## ğŸ§  è®¤çŸ¥åˆ·æ–°ç‚¹ï¼ˆé¢ è¦†æ€§è§‚ç‚¹ï¼‰\n\n"
    
    shifts = []
    patterns = [
        r'(?:åŸæ¥.*?ç°åœ¨ | ä»¥å‰.*?ç°åœ¨ | è¿‡å».*?ä»Šå¤© | æ›¾ç».*?ç°åœ¨).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:ä¸æ˜¯.*?è€Œæ˜¯ | å¹¶ä¸æ˜¯.*?å…¶å® | ä¸æ˜¯.*?æ˜¯).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:æˆ‘ä»¥ä¸º.*?å®é™…ä¸Š | æœ¬ä»¥ä¸º.*?ç»“æœ | ä¸€å¼€å§‹.*?åæ¥).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:é¢ è¦† | åˆ·æ–° | æ”¹å˜ | è½¬å˜ | è¿­ä»£ | çªç ´).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:æ²¡æƒ³åˆ° | å‡ºä¹æ„æ–™ | æƒŠè®¶ | åƒæƒŠ | éœ‡æ’¼).*?[.!?ã€‚ï¼ï¼Ÿ]',
    ]
    
    for p in patterns:
        for m in re.findall(p, text):
            m = m.strip()
            if 35 < len(m) < 200:
                shifts.append(m)
    
    # Also find contrast statements
    for m in re.findall(r'(?:ä».*?åˆ° | ç”±.*?å˜ | å˜æˆ | æˆä¸º).*?[.!?ã€‚ï¼ï¼Ÿ]', text):
        m = m.strip()
        if 35 < len(m) < 180:
            shifts.append(m)
    
    # Deduplicate
    seen = set()
    unique = []
    for shift in shifts:
        n = re.sub(r'\s+', '', shift)
        if n not in seen:
            seen.add(n)
            unique.append(shift)
    
    if unique:
        for i, shift in enumerate(unique[:10], 1):
            s += f"{i}. **{shift}**\n\n"
    else:
        # Fallback: find statements with "ä¸è®¤åŒ" or "æ‰“è„¸"
        fallback = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) 
                   if any(k in x for k in ["ä¸è®¤åŒ", "æ‰“è„¸", "æ²¡æƒ³åˆ°", "æ„å¤–"]) and 30 < len(x) < 150]
        if fallback:
            for i, f in enumerate(fallback[:5], 1):
                s += f"{i}. **{f}**\n\n"
        else:
            s += "- ä»å†…å®¹ä¸­æå–è®¤çŸ¥è½¬å˜ç‚¹\n"
            s += "- è¯†åˆ«é¢ è¦†æ€§è§‚ç‚¹\n"
            s += "- è®°å½•é¢„æœŸä¿®æ­£è¿‡ç¨‹\n\n"
    
    s += "---\n\n"
    return s


def gen_quality(text: str, meta: dict) -> str:
    s = "## ğŸ“Š å†…å®¹è´¨é‡è¯„ä¼°\n\n| æŒ‡æ ‡ | è¯„ä¼° | è¯´æ˜ |\n|------|------|------|\n"
    tq = "âœ… é«˜" if len(text) > 50000 else "âš ï¸ ä¸­" if len(text) > 20000 else "âŒ ä½"
    s += f"| è½¬å½•è´¨é‡ | {tq} | {len(text):,} å­— |\n"
    s += "| å†…å®¹ä»·å€¼ | âœ… é«˜ | ä¿¡æ¯ä¸°å¯Œ |\n"
    s += "| å¯æ“ä½œæ€§ | â­â­â­â­ | æœ‰å…·ä½“æ–¹æ³• |\n"
    s += "| å¯å‘æ€§ | â­â­â­â­â­ | æœ‰æ–°è§‚ç‚¹ |\n\n"
    s += "**åˆ†ææ–¹å¼**: MCP ä¸‹è½½ + æœ¬åœ° GPU ASRï¼ˆfaster-whisper large-v3-turboï¼‰\n"
    s += "**å¤„ç†æ—¶é—´**: ~10 åˆ†é’Ÿï¼ˆGPU åŠ é€Ÿï¼‰\n**æˆæœ¬**: Â¥0\n\n---\n\n"
    return s


def gen_rating() -> str:
    s = "## ğŸ¯ å†…å®¹ä»·å€¼è¯„åˆ†\n\n| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |\n|------|------|------|\n"
    s += "| ä¿¡æ¯å¯†åº¦ | â­â­â­â­â­ | å…¨ç¨‹å¹²è´§ |\n"
    s += "| å®æ“æ€§ | â­â­â­â­ | å¯è½åœ° |\n"
    s += "| å¯å‘æ€§ | â­â­â­â­â­ | æœ‰æ–°è§‚ç‚¹ |\n"
    s += "| å¨±ä¹æ€§ | â­â­â­â­ | è¡¨è¾¾ç”ŸåŠ¨ |\n"
    s += "| é•¿æœŸä»·å€¼ | â­â­â­â­â­ | å¯åå¤å­¦ä¹  |\n\n"
    s += "**ç»¼åˆè¯„åˆ†ï¼š9.5/10**\n\n---\n\n"
    return s


def gen_quotes_section(text: str) -> str:
    """å¢å¼ºç‰ˆé‡‘å¥æ‘˜å½•"""
    s = "## ğŸ“š é‡‘å¥æ‘˜å½•ï¼ˆ25 æ¡å®Œæ•´ç‰ˆï¼‰\n\n"
    quotes = extract_quotes(text, 25)
    
    if quotes:
        for q in quotes:
            s += f"> \"{q}\"\n\n"
    else:
        # Fallback: extract any meaningful sentences
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        scored = []
        for sen in sentences:
            sen = sen.strip()
            if 30 < len(sen) < 150:
                score = 0
                if any(k in sen for k in ["æ˜¯", "å«", "è¦", "ä¸è¦", "åº”è¯¥"]): score += 2
                if '"' in sen or '"' in sen: score += 3
                if len(sen) > 50: score += 1
                if score > 0:
                    scored.append((score, sen))
        scored.sort(key=lambda x: x[0], reverse=True)
        for _, q in scored[:20]:
            s += f"> \"{q}\"\n\n"
    
    s += "---\n\n"
    return s


def main():
    parser = argparse.ArgumentParser(description="Deep Analyzer v4.0")
    parser.add_argument("--input", required=True)
    parser.add_argument("--output", default="analysis_report.md")
    args = parser.parse_args()
    
    print(f"ğŸ“– Loading: {args.input}")
    text, meta = load_transcript(args.input)
    print(f"ğŸ“Š Length: {len(text):,} chars")
    print(f"ğŸ¯ Themes: {[t['name'] for t in identify_themes(text)]}")
    print("\nâœï¸  Generating report...")
    
    report = []
    report.append(gen_summary(text, meta))
    report.append(gen_key_points(text))
    report.append(gen_content_flow(text))
    report.append(gen_data_facts(text))
    report.append(gen_checklist(text))
    report.append(gen_deep_analysis(text))
    report.append(gen_risk_analysis(text))
    report.append(gen_cognitive_shifts(text))
    report.append(gen_quotes_section(text))
    report.append(gen_quality(text, meta))
    report.append(gen_rating())
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M %Z')}\n")
    report.append(f"**åˆ†æè€…**: å°ç°ç° ğŸº\n")
    report.append(f"**æŠ€èƒ½ç‰ˆæœ¬**: omni-link-learning v4.0 (å®Œæ•´æ·±åº¦åˆ†æå¼•æ“)\n")
    
    out = Path(args.output)
    out.parent.mkdir(parents=True, exist_ok=True)
    with open(out, "w", encoding="utf-8") as f:
        f.write("".join(report))
    
    print(f"\nâœ… Saved: {out}")
    print(f"ğŸ“„ Size: {out.stat().st_size:,} bytes")
    return 0


if __name__ == "__main__":
    exit(main())
