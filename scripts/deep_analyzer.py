#!/usr/bin/env python3
"""
Deep Analyzer v4.0 - å®Œæ•´æ·±åº¦åˆ†æå¼•æ“
æ ¸å¿ƒç†å¿µï¼šä¸é—æ¼ä»»ä½•æœ‰ä»·å€¼çš„å†…å®¹

v4.0 æ–°å¢:
- å…³é”®è¦ç‚¹ 8â†’15 ä¸ª
- é‡‘å¥ 15â†’25 ä¸ª
- æ–°å¢"å®Œæ•´å†…å®¹è„‰ç»œ"ç« èŠ‚
- æ–°å¢"å…³é”®æ•°æ®ä¸äº‹å®æå–"
- æ–°å¢"å®æˆ˜åº”ç”¨æ¸…å•"
- æ–°å¢"è®¤çŸ¥åˆ·æ–°ç‚¹"
- å¢å¼ºæ™ºèƒ½å¡«å……
"""

import argparse
import json
import re
from pathlib import Path
from datetime import datetime


def load_transcript(input_path: str) -> tuple[str, dict]:
    """Load transcript and metadata."""
    path = Path(input_path)
    meta_path = path.parent / "douyin_mcp_result.json"
    metadata = {}
    
    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
            video_info = meta.get("video_info", {})
            if isinstance(video_info, dict):
                metadata["title"] = video_info.get("title", "Unknown")
                metadata["author"] = video_info.get("author", "Unknown")
                metadata["platform"] = meta.get("platform", "Unknown")
            
            if "transcript" in meta and meta["transcript"]:
                try:
                    data = json.loads(meta["transcript"])
                    if isinstance(data, dict):
                        if "text" in data:
                            return data["text"].strip(), metadata
                        elif "segments" in data:
                            text = "".join([s.get("text", "") for s in data["segments"]])
                            return text.strip(), metadata
                except: pass
    
    with open(path, "r", encoding="utf-8") as f:
        content = f.read().strip()
    
    if content.startswith('{'):
        try:
            data = json.loads(content)
            if isinstance(data, dict):
                if "text" in data:
                    return data["text"].strip(), metadata
                elif "segments" in data:
                    text = "".join([s.get("text", "") for s in data["segments"]])
                    return text.strip(), metadata
        except: pass
    
    return content, metadata


def identify_themes(text: str) -> list:
    themes = {
        "èŒåœºæˆé•¿": ["åŠªåŠ›", "è§„åˆ’", "æœºä¼š", "è·³æ§½", "æ·±è€•", "é•¿æœŸä¸»ä¹‰"],
        "é”€å”®æŠ€å·§": ["æ‹œè®¿", "å®¢æˆ·", "ä¿¡ä»»", "æˆäº¤", "é™Œæ‹œ", "ä¸šç»©"],
        "è‡ªåª’ä½“": ["æµé‡", "ç²‰ä¸", "è§†é¢‘", "çˆ†æ¬¾", "ç®—æ³•", "è·å®¢"],
        "AI ä¸æŠ€æœ¯": ["AI", "å·¥å…·", "è‡ªåŠ¨åŒ–", "æ¨¡å‹", "Agent"],
        "æŠ•èµ„æ€ç»´": ["æŠ•èµ„", "å‘¨æœŸ", "éå…±è¯†", "åˆ›å§‹äºº", "æœºä¼š"],
        "å•†ä¸šæ´å¯Ÿ": ["å¸‚åœº", "ç«äº‰", "åˆ©æ¶¦", "æ¨¡å¼", "ç”Ÿæ€"],
    }
    result = []
    for name, kws in themes.items():
        count = sum(text.count(k) for k in kws)
        if count > 3:
            result.append({"name": name, "count": count})
    result.sort(key=lambda x: x["count"], reverse=True)
    return result[:6]


def split_sentences(text: str) -> list:
    """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ - å¤„ç†æ— æ ‡ç‚¹ ASR è½¬å½•"""
    # First try normal sentence splitting
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
    
    # If too few sentences, try semantic splitting
    if len(sentences) < 10:
        # Split by common connectors
        connectors = [' å› ä¸º ', ' æ‰€ä»¥ ', ' ä½†æ˜¯ ', ' ç„¶å ', ' å°± ', ' é‚£ ', ' å¯¹ ', ' å…¶å® ', ' æˆ‘è§‰å¾— ', ' æˆ‘è®¤ä¸º ']
        result = [text]
        for conn in connectors:
            new_result = []
            for s in result:
                if len(s) > 300:  # Only split long segments
                    parts = s.split(conn)
                    for i, p in enumerate(parts):
                        if i > 0 and len(p) > 20:
                            new_result.append(conn.strip() + p)
                        elif len(p) > 20:
                            new_result.append(p)
                else:
                    new_result.append(s)
            result = new_result
        
        # Split by length (max 200 chars)
        final = []
        for s in result:
            if len(s) > 200:
                # Split at natural pauses
                for i in range(0, len(s), 150):
                    chunk = s[i:i+150]
                    if len(chunk) > 30:
                        final.append(chunk)
            elif len(s) > 30:
                final.append(s)
        return final
    
    # Filter and clean
    cleaned = []
    for s in sentences:
        s = s.strip()
        if len(s) > 20:
            cleaned.append(s)
    return cleaned


def extract_quotes(text: str, max_q: int = 25) -> list:
    """å¢å¼ºç‰ˆé‡‘å¥æå– - å¤„ç† ASR è½¬å½•"""
    quotes = []
    sentences = split_sentences(text)
    
    # Pattern 1: Direct quotes
    for s in sentences:
        if '"' in s or '"' in s:
            matches = re.findall(r'[""](.*?)[""]', s)
            for m in matches:
                if 20 < len(m) < 150:
                    quotes.append(m.strip())
    
    # Pattern 2: Importance markers
    markers = ["æœ€é‡è¦çš„æ˜¯", "å…³é”®æ˜¯", "æ ¸å¿ƒ", "è®°ä½", "ä¸€å®šè¦", "æœ¬è´¨ä¸Š", "æˆ‘è®¤ä¸º", 
               "æˆ‘è§‰å¾—", "æˆ‘å°è±¡", "æˆ‘å‘ç°", "æˆ‘çš„è§‚ç‚¹", "è¯´ç™½äº†", "å¬å¥½", "æˆ‘è·Ÿä½ è®²",
               "æˆ‘çš„æ„Ÿå—", "æˆ‘è‡ªå·±", "åœ¨æˆ‘çœ‹æ¥"]
    for s in sentences:
        if any(m in s for m in markers):
            if 30 < len(s) < 200:
                quotes.append(s)
    
    # Pattern 3: Contrast patterns
    for s in sentences:
        if ("ä¸æ˜¯" in s and "è€Œæ˜¯" in s) or ("ä»" in s and "åˆ°" in s and len(s) > 40):
            if 40 < len(s) < 200:
                quotes.append(s)
    
    # Pattern 4: Definition patterns  
    for s in sentences:
        if any(k in s for k in ["å«åš", "å°±æ˜¯", "ç­‰äº", "æ„å‘³ç€", "æ˜¯ç¬¬ä¸€ä¸ª"]):
            if 30 < len(s) < 180:
                quotes.append(s)
    
    # Pattern 5: Advice patterns
    for s in sentences:
        if any(k in s for k in ["è¦ ", "ä¸è¦ ", "åº”è¯¥ ", "å¿…é¡» ", "å¯ä»¥ "]):
            if 25 < len(s) < 180 and len(s.split(' ')) < 30:
                quotes.append(s)
    
    # Pattern 6: Insight patterns
    for s in sentences:
        if any(k in s for k in ["éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "æ„å¤–", "é¢ è¦†", "åˆ·æ–°", "çªç ´"]):
            if 30 < len(s) < 180:
                quotes.append(s)
    
    # Deduplicate
    seen = set()
    unique = []
    for q in quotes:
        n = re.sub(r'\s+', '', q)
        if n not in seen and len(q) > 25:
            seen.add(n)
            unique.append(q)
    
    # Sort by length and quality
    unique.sort(key=lambda x: (-len(x), x))
    return unique[:max_q]


def extract_data(text: str) -> list:
    data = []
    for m in re.findall(r'(\d+(?:\.\d+)?(?:ä¸‡ | äº¿ | å€ | å¹´ | ä¸ªæœˆ |%|ï¼…))', text):
        idx = text.find(m)
        ctx = text[max(0,idx-40):min(len(text),idx+len(m)+40)].strip()
        data.append({"type": "æ•°æ®", "value": m, "context": ctx})
    for m in re.findall(r'(å­—èŠ‚ | æŠ–éŸ³ | è…¾è®¯ | é˜¿é‡Œ | ç¾å›¢ | å°çº¢ä¹¦ |Google|Meta|OpenAI|Midjourney)', text):
        data.append({"type": "æ¡ˆä¾‹", "value": m, "context": ""})
    seen = set()
    unique = []
    for d in data:
        k = (d["type"], d["value"])
        if k not in seen:
            seen.add(k)
            unique.append(d)
    return unique[:25]


def extract_advice(text: str) -> list:
    advice = []
    for p in [r'è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'ä¸è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'åº”è¯¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', r'å¿…é¡» (.*?)[.!?ã€‚ï¼ï¼Ÿ]']:
        for m in re.findall(p, text):
            m = m.strip()
            if 15 < len(m) < 200:
                advice.append(m)
    seen = set()
    unique = []
    for a in advice:
        if a[:50] not in seen:
            seen.add(a[:50])
            unique.append(a)
    return unique[:20]


def gen_summary(text: str, meta: dict) -> str:
    title = meta.get("title", "æœªå‘½å")
    platform = meta.get("platform", "Unknown")
    themes = identify_themes(text)
    theme_str = "ã€".join([t["name"] for t in themes]) if themes else "ç»¼åˆå†…å®¹"
    quotes = extract_quotes(text, 4)
    
    s = f"# ğŸ“Š å®Œæ•´åˆ†ææŠ¥å‘Š\n\n## ğŸ“‹ è§†é¢‘å…ƒæ•°æ®\n\n"
    s += f"- **æ¥æº**: {platform}\n- **æ ‡é¢˜**: {title}\n- **è½¬å½•é•¿åº¦**: {len(text):,} å­—\n"
    s += f"- **è§†é¢‘æ—¶é•¿**: çº¦ {len(text)//250} åˆ†é’Ÿ\n- **åˆ†ææ–¹æ³•**: MCP ä¸‹è½½ + æœ¬åœ° GPU ASR\n\n---\n\n"
    s += f"## ğŸ¯ æ ¸å¿ƒæ‘˜è¦ï¼ˆ30 ç§’é€Ÿè¯»ï¼‰\n\næœ¬è§†é¢‘æ ¸å¿ƒä¸»é¢˜ï¼š**{theme_str}**\n\n"
    s += "**æ ¸å¿ƒè§‚ç‚¹**:\n"
    for q in quotes:
        s += f"> \"{q}\"\n\n"
    s += "**ä¸ºä»€ä¹ˆå€¼å¾—çœ‹**:\n- âœ… å®æˆ˜ç»éªŒï¼Œéç†è®ºç©ºè°ˆ\n- âœ… å…·ä½“æ–¹æ³•è®º\n- âœ… æœ‰æ¡ˆä¾‹æ”¯æ’‘\n- âœ… æœ‰æ•°æ®éªŒè¯\n\n---\n\n"
    return s


def gen_key_points(text: str) -> str:
    """å¢å¼ºç‰ˆå…³é”®è¦ç‚¹æå– - å¤„ç† ASR è½¬å½•"""
    s = "## ğŸ“ å…³é”®è¦ç‚¹æ·±åº¦è§£è¯»ï¼ˆ15 ä¸ªå®Œæ•´ç‰ˆï¼‰\n\n"
    sentences = split_sentences(text)
    kws = ["æœ€é‡è¦çš„æ˜¯", "å…³é”®æ˜¯", "æ ¸å¿ƒ", "è®°ä½", "ä¸€å®šè¦", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰", "æ€»ç»“", 
           "æœ¬è´¨ä¸Š", "æˆ‘è®¤ä¸º", "æˆ‘è§‰å¾—", "å…¬å¼", "æ³•åˆ™", "æ­¥éª¤", "æ–¹æ³•", "å¬å¥½", "è¯´ç™½äº†",
           "éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "é¢ è¦†", "åˆ·æ–°", "çªç ´", "æˆ‘çš„æ„Ÿå—", "åœ¨æˆ‘çœ‹æ¥"]
    
    scored = []
    for i, sen in enumerate(sentences):
        sen = sen.strip()
        if len(sen) < 30 or len(sen) > 300: continue
        score = sum(3 for k in kws if k in sen)
        if re.search(r'\d+', sen): score += 2
        if any(w in sen for w in ["è¦ ", "ä¸è¦ ", "åº”è¯¥ ", "å¿…é¡» "]): score += 2
        if '"' in sen or '"' in sen: score += 3
        if "ä¸æ˜¯" in sen and "è€Œæ˜¯" in sen: score += 3
        if "ä»" in sen and "åˆ°" in sen: score += 2
        if any(k in sen for k in ["éœ‡æ’¼", "æƒŠå–œ", "æ²¡æƒ³åˆ°", "é¢ è¦†"]): score += 3
        if score > 0:
            scored.append((score, sen, i))
    
    scored.sort(key=lambda x: x[0], reverse=True)
    
    for i, (score, point, idx) in enumerate(scored[:15], 1):
        point = point.strip()
        if point.startswith(('ï¼Œ', 'ã€‚', 'ã€', ' ')):
            point = point.lstrip('ï¼Œã€‚ã€ ')
        
        s += f"### {i}. {point}\n\n"
        s += "**æ·±åº¦è§£è¯»**:\n"
        
        # Find context
        ctx_start = max(0, idx - 2)
        ctx_end = min(len(sentences), idx + 3)
        context = " ".join([sentences[j].strip() for j in range(ctx_start, ctx_end) if len(sentences[j].strip()) > 20])
        
        if any(k in point for k in ["æ–¹æ³•", "æ­¥éª¤", "æ€ä¹ˆ", "å¦‚ä½•", "å…¬å¼", "ç¬¬ä¸€", "ç¬¬äºŒ", "ç¬¬ä¸‰"]):
            s += "- ğŸ”§ **æ–¹æ³•è®º**: è¿™æ˜¯ä¸€ä¸ªå…·ä½“çš„æ“ä½œæ–¹æ³•\n"
            if context: s += f"- ğŸ“– **ä¸Šä¸‹æ–‡**: {context[:150]}...\n"
            s += "- âœ… **æ‰§è¡Œè¦ç‚¹**: æ³¨æ„å…³é”®æ‰§è¡Œç»†èŠ‚\n\n"
        elif any(k in point for k in ["ä¸è¦", "é¿å…", "é£é™©", "ä¸èƒ½", "åƒä¸‡åˆ«", "æ— æ³•"]):
            s += "- âš ï¸ **è­¦ç¤º**: è¿™æ˜¯ä¸€ä¸ªéœ€è¦æ³¨æ„çš„é£é™©ç‚¹\n"
            s += "- ğŸ” **é£é™©æ¥æº**: è¯†åˆ«é£é™©çš„æ ¹æº\n"
            s += "- ğŸ›¡ï¸ **è§„é¿æ–¹æ³•**: å¦‚ä½•é¿å…è¿™ä¸ªé£é™©\n\n"
        elif any(k in point for k in ["è¦ ", "åº”è¯¥ ", "å¿…é¡» ", "ä¸€å®š"]):
            s += "- âœ… **è¡ŒåŠ¨æŒ‡å—**: è¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„è¡ŒåŠ¨å»ºè®®\n"
            for j in range(idx, min(len(sentences), idx + 3)):
                if any(p in sentences[j] for p in ["å› ä¸º", "æ‰€ä»¥", "å¦åˆ™", "ä¸ç„¶", "æ‰èƒ½"]):
                    s += f"- ğŸ’¡ **åŸå› **: {sentences[j].strip()[:120]}...\n"
                    break
            s += "- ğŸ“‹ **å¦‚ä½•æ‰§è¡Œ**: æ‹†è§£ä¸ºå…·ä½“æ­¥éª¤\n\n"
        elif "ä¸æ˜¯" in point and "è€Œæ˜¯" in point:
            s += "- ğŸ”„ **å¯¹æ¯”/çº æ­£**: è¿™æ˜¯ä¸€ä¸ªè®¤çŸ¥çº æ­£\n"
            s += "- âŒ **å¸¸è§è¯¯åŒº**: äººä»¬é€šå¸¸æ€ä¹ˆæƒ³\n"
            s += "- âœ… **æ­£ç¡®ç†è§£**: å®é™…åº”è¯¥æ€ä¹ˆçœ‹\n\n"
        elif any(k in point for k in ["å«åš", "å°±æ˜¯", "ç­‰äº", "æ„å‘³ç€", "æ˜¯ç¬¬ä¸€ä¸ª"]):
            s += "- ğŸ’ **å®šä¹‰/æ´å¯Ÿ**: è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µæˆ–æ´å¯Ÿ\n"
            if context: s += f"- ğŸ“– **èƒŒæ™¯**: {context[:120]}...\n"
            s += "- ğŸ¯ **åº”ç”¨**: å¦‚ä½•åº”ç”¨åˆ°ä½ çš„æƒ…å†µ\n\n"
        else:
            s += "- ğŸ’¡ **è§‚ç‚¹**: è¿™æ˜¯ä¸€ä¸ªæ´å¯Ÿæˆ–è§‚ç‚¹\n"
            if context: s += f"- ğŸ“– **èƒŒæ™¯**: {context[:120]}...\n"
            s += "- ğŸ¯ **åº”ç”¨**: å¦‚ä½•åº”ç”¨åˆ°ä½ çš„æƒ…å†µ\n\n"
        
        s += "---\n\n"
    return s


def gen_content_flow(text: str) -> str:
    s = "## ğŸ“– å®Œæ•´å†…å®¹è„‰ç»œï¼ˆæŒ‰é€»è¾‘é¡ºåºï¼‰\n\n"
    sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
    chunk_size = 15
    chunks = []
    for i in range(0, len(sentences), chunk_size):
        chunk = " ".join([s.strip() for s in sentences[i:i+chunk_size] if len(s.strip()) > 10])
        if len(chunk) > 50:
            chunks.append(chunk[:400])
    
    for i, chunk in enumerate(chunks[:8], 1):
        s += f"**ç¬¬{i}éƒ¨åˆ†**: {chunk}...\n\n"
    s += "---\n\n"
    return s


def gen_data_facts(text: str) -> str:
    s = "## ğŸ“Š å…³é”®æ•°æ®ä¸äº‹å®æå–\n\n"
    data = extract_data(text)
    by_type = {}
    for d in data:
        t = d["type"]
        if t not in by_type: by_type[t] = []
        by_type[t].append(d)
    
    for t, items in by_type.items():
        s += f"**{t}**:\n"
        for item in items[:8]:
            if item["context"]:
                s += f"- `{item['value']}` â€” {item['context'][:80]}...\n"
            else:
                s += f"- `{item['value']}`\n"
        s += "\n"
    s += "---\n\n"
    return s


def gen_checklist(text: str) -> str:
    """å¢å¼ºç‰ˆå®æˆ˜æ¸…å•"""
    s = "## âœ… å®æˆ˜åº”ç”¨æ¸…å•ï¼ˆå¯ç›´æ¥æ‰§è¡Œï¼‰\n\n"
    
    advice = []
    # Extract actionable advice
    patterns = [
        r'è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ä¸è¦ (.*?)[.!?ã€‚ï¼ï¼Ÿ]', 
        r'åº”è¯¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'å¿…é¡» (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'å¯ä»¥ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ç¬¬ä¸€æ­¥ [ï¼Œ,]*(.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'é¦–å…ˆ (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'ç„¶å (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
        r'æœ€å (.*?)[.!?ã€‚ï¼ï¼Ÿ]',
    ]
    
    for p in patterns:
        for m in re.findall(p, text):
            m = m.strip()
            if 15 < len(m) < 200:
                advice.append(m)
    
    # Deduplicate
    seen = set()
    unique = []
    for a in advice:
        if a[:50] not in seen:
            seen.add(a[:50])
            unique.append(a)
    
    if unique:
        for a in unique[:20]:
            s += f"- [ ] {a}\n"
    else:
        s += "- ä»å†…å®¹ä¸­æå–å¯æ‰§è¡Œå»ºè®®\n"
        s += "- æ•´ç†ä¸ºè¡ŒåŠ¨æ¸…å•\n"
    
    s += "\n---\n\n"
    return s


def analyze_insight_deep(insight: str, text: str) -> dict:
    """æ·±åº¦åˆ†æå•æ¡æ´å¯Ÿï¼šè§£è¯» + åº•å±‚é€»è¾‘ + æœªæ¥å¯ç¤º"""
    result = {
        "è§£è¯»": "",
        "åº•å±‚é€»è¾‘": "",
        "æœªæ¥å¯ç¤º": ""
    }
    
    # æ·±åº¦è§£è¯» - ä¸ºä»€ä¹ˆé‡è¦
    if any(k in insight for k in ["ä¸è¦", "ä¸èƒ½", "é¿å…", "é£é™©", "é™·é˜±"]):
        result["è§£è¯»"] = "è¿™æ˜¯ä¸€ä¸ªéœ€è¦è­¦æƒ•çš„é£é™©ç‚¹ï¼Œè¯†åˆ«æ½œåœ¨é™·é˜±å¯é¿å…é‡å¤§æŸå¤±"
    elif any(k in insight for k in ["è¦", "åº”è¯¥", "å¿…é¡»", "ä¸€å®š"]):
        result["è§£è¯»"] = "è¿™æ˜¯ä¸€ä¸ªæ˜ç¡®çš„è¡ŒåŠ¨æŒ‡å—ï¼ŒæŒ‡å‡ºäº†æ­£ç¡®çš„æ–¹å‘å’Œæ–¹æ³•"
    elif any(k in insight for k in ["ä¸æ˜¯", "è€Œæ˜¯", "å¹¶ä¸æ˜¯", "å…¶å®æ˜¯"]):
        result["è§£è¯»"] = "è¿™æ˜¯ä¸€ä¸ªè®¤çŸ¥çº æ­£ï¼Œæ‰“ç ´äº†å¸¸è§çš„æ€ç»´è¯¯åŒº"
    elif any(k in insight for k in ["å› ä¸º", "æ‰€ä»¥", "å› æ­¤", "å¯¼è‡´"]):
        result["è§£è¯»"] = "è¿™æ˜¯ä¸€ä¸ªå› æœåˆ†æï¼Œæ­ç¤ºäº†äº‹ç‰©ä¹‹é—´çš„é€»è¾‘å…³ç³»"
    elif any(k in insight for k in ["é¢„æµ‹", "æœŸå¾…", "æœªæ¥", "æ˜å¹´", "å°†ä¼š"]):
        result["è§£è¯»"] = "è¿™æ˜¯ä¸€ä¸ªæœªæ¥é¢„åˆ¤ï¼ŒåŸºäºå½“å‰è¶‹åŠ¿çš„å‰ç»æ€§æ€è€ƒ"
    elif any(k in insight for k in ["æœºä¼š", "èµ›é“", "æ–¹å‘", "åˆ›ä¸š", "æŠ•èµ„"]):
        result["è§£è¯»"] = "è¿™æ˜¯ä¸€ä¸ªæœºä¼šè¯†åˆ«ï¼ŒæŒ‡å‡ºäº†å€¼å¾—å…³æ³¨çš„é¢†åŸŸ"
    else:
        result["è§£è¯»"] = "è¿™æ˜¯ä¸€ä¸ªæ ¸å¿ƒæ´å¯Ÿï¼Œåæ˜ äº†å¯¹äº‹ç‰©æœ¬è´¨çš„ç†è§£"
    
    # åº•å±‚é€»è¾‘ - èƒŒåçš„åŸç†
    if any(k in insight for k in ["äºº", "ç”¨æˆ·", "æ¶ˆè´¹è€…", "å¿ƒç†", "éœ€æ±‚"]):
        result["åº•å±‚é€»è¾‘"] = "åŸºäºäººæ€§åº•å±‚éœ€æ±‚ï¼šè¢«çœ‹è§ã€è¢«è®¤å¯ã€è¢«ç†è§£ã€å½’å±æ„Ÿã€æˆå°±æ„Ÿ"
    elif any(k in insight for k in ["ä»·å€¼", "ä»·æ ¼", "æˆæœ¬", "æ”¶ç›Š", "åˆ©æ¶¦"]):
        result["åº•å±‚é€»è¾‘"] = "åŸºäºä»·å€¼åˆ›é€ ä¸åˆ†é…ï¼šè§£å†³ç—›ç‚¹â†’åˆ›é€ ä»·å€¼â†’è·å–å›æŠ¥"
    elif any(k in insight for k in ["ç«äº‰", "å£å’", "æŠ¤åŸæ²³", "ä¼˜åŠ¿"]):
        result["åº•å±‚é€»è¾‘"] = "åŸºäºç«äº‰æˆ˜ç•¥ï¼šå·®å¼‚åŒ–å®šä½â†’å»ºç«‹å£å’â†’æŒç»­é¢†å…ˆ"
    elif any(k in insight for k in ["è§„æ¨¡", "ç½‘ç»œ", "å¹³å°", "ç”Ÿæ€"]):
        result["åº•å±‚é€»è¾‘"] = "åŸºäºç½‘ç»œæ•ˆåº”ï¼šç”¨æˆ·å¢é•¿â†’ä»·å€¼æå‡â†’æ›´å¼ºå¢é•¿"
    elif any(k in insight for k in ["è¶‹åŠ¿", "å‘¨æœŸ", "æ—¶æœº", "çª—å£"]):
        result["åº•å±‚é€»è¾‘"] = "åŸºäºè¶‹åŠ¿åˆ¤æ–­ï¼šé¡ºåŠ¿è€Œä¸ºâ†’æŠ“ä½çª—å£â†’å¿«é€Ÿæ‰§è¡Œ"
    else:
        result["åº•å±‚é€»è¾‘"] = "åŸºäºç¬¬ä¸€æ€§åŸç†ï¼šä»æœ¬è´¨å‡ºå‘ï¼Œæ¨å¯¼æœ€ä¼˜è§£"
    
    # æœªæ¥å¯ç¤º - å¯¹æœªæ¥çš„æŒ‡å¯¼
    if any(k in insight for k in ["AI", "æ¨¡å‹", "æŠ€æœ¯", "äº§å“"]):
        result["æœªæ¥å¯ç¤º"] = "AI æ—¶ä»£ç”Ÿå­˜æ³•åˆ™ï¼šæ‹¥æŠ±å˜åŒ– + æŒç»­å­¦ä¹  + æ‰¾åˆ°äººä¸ AI çš„åä½œç‚¹"
    elif any(k in insight for k in ["åˆ›ä¸š", "æŠ•èµ„", "æœºä¼š", "èµ›é“"]):
        result["æœªæ¥å¯ç¤º"] = "åˆ›ä¸š/æŠ•èµ„å»ºè®®ï¼šé€‰æ‹©è¶³å¤Ÿå¤§çš„èµ›é“ï¼Œä¿æŒä¹è§‚ï¼Œå¿«é€Ÿè¿­ä»£"
    elif any(k in insight for k in ["èŒåœº", "èƒ½åŠ›", "æˆé•¿", "å‘å±•"]):
        result["æœªæ¥å¯ç¤º"] = "ä¸ªäººå‘å±•å»ºè®®ï¼šåŸ¹å…»ä¸å¯æ›¿ä»£çš„èƒ½åŠ›ï¼Œå»ºç«‹ä¸ªäººæŠ¤åŸæ²³"
    elif any(k in insight for k in ["ç»„ç»‡", "å›¢é˜Ÿ", "å…¬å¸", "ç®¡ç†"]):
        result["æœªæ¥å¯ç¤º"] = "ç»„ç»‡å»ºè®¾å»ºè®®ï¼šå°å›¢é˜Ÿ+AI æ æ†ï¼Œæ‰å¹³åŒ–ï¼Œå¿«é€Ÿå†³ç­–"
    else:
        result["æœªæ¥å¯ç¤º"] = "è¡ŒåŠ¨å»ºè®®ï¼šå°†æ´å¯Ÿè½¬åŒ–ä¸ºå…·ä½“è¡ŒåŠ¨ï¼ŒæŒç»­éªŒè¯å’Œè¿­ä»£"
    
    return result


def gen_deep_analysis(text: str) -> str:
    """å¢å¼ºç‰ˆæ·±åº¦åˆ†æ - åŸæ–‡ + è§£è¯» + åº•å±‚é€»è¾‘ + æœªæ¥å¯ç¤º"""
    s = "## ğŸ’¡ æ·±åº¦åˆ†æä¸æ´å¯Ÿ\n\n"
    
    sentences = split_sentences(text)
    
    # ========== äººæ€§å±‚é¢ ==========
    human_kws = ["äºº", "äººæ€§", "å¿ƒç†", "éœ€è¦", "æƒ³è¦", "å®³æ€•", "è§‰å¾—", "æ„Ÿè§‰", "å¸Œæœ›", "ç”¨æˆ·", "æ¶ˆè´¹è€…"]
    human = [x for x in sentences if any(k in x for k in human_kws) and 50 < len(x) < 350]
    
    s += "### ä¸€ã€äººæ€§å±‚é¢æ´å¯Ÿ (ä»åŸæ–‡æå– {0} æ¡)\n\n".format(len(human))
    if human:
        seen = set()
        count = 0
        for h in human:
            if count >= 8: break
            norm = re.sub(r'\s+', '', h[:80])
            if norm in seen: continue
            seen.add(norm)
            count += 1
            analysis = analyze_insight_deep(h, text)
            s += f"**{count}. åŸæ–‡**:\n{h}\n\n"
            s += f"**æ·±åº¦è§£è¯»**: {analysis['è§£è¯»']}\n\n"
            s += f"**åº•å±‚é€»è¾‘**: {analysis['åº•å±‚é€»è¾‘']}\n\n"
            s += f"**æœªæ¥å¯ç¤º**: {analysis['æœªæ¥å¯ç¤º']}\n\n"
            s += "---\n\n"
    else:
        s += "æš‚æ— è¶³å¤Ÿæ´å¯Ÿ\n\n"
    
    # ========== å•†ä¸š/ä»·å€¼å±‚é¢ ==========
    biz_kws = ["ä»·å€¼", "åˆ©ç›Š", "æˆæœ¬", "æ”¶ç›Š", "æ•ˆç‡", "åˆ©æ¶¦", "èµšé’±", "ç”Ÿæ„", "å•†ä¸š", "å¸‚åœº", "ç«äº‰", "å£å’", "æŠ¤åŸæ²³", "å•†ä¸šæ¨¡å¼"]
    biz = [x for x in sentences if any(k in x for k in biz_kws) and 50 < len(x) < 350]
    
    s += "### äºŒã€å•†ä¸š/ä»·å€¼å±‚é¢æ´å¯Ÿ (ä»åŸæ–‡æå– {0} æ¡)\n\n".format(len(biz))
    if biz:
        seen = set()
        count = 0
        for b in biz:
            if count >= 8: break
            norm = re.sub(r'\s+', '', b[:80])
            if norm in seen: continue
            seen.add(norm)
            count += 1
            analysis = analyze_insight_deep(b, text)
            s += f"**{count}. åŸæ–‡**:\n{b}\n\n"
            s += f"**æ·±åº¦è§£è¯»**: {analysis['è§£è¯»']}\n\n"
            s += f"**åº•å±‚é€»è¾‘**: {analysis['åº•å±‚é€»è¾‘']}\n\n"
            s += f"**æœªæ¥å¯ç¤º**: {analysis['æœªæ¥å¯ç¤º']}\n\n"
            s += "---\n\n"
    else:
        s += "æš‚æ— è¶³å¤Ÿæ´å¯Ÿ\n\n"
    
    # ========== ç³»ç»Ÿ/æ¨¡å¼å±‚é¢ ==========
    sys_kws = ["ç³»ç»Ÿ", "å¾ªç¯", "æ æ†", "è§„æ¨¡", "æ¨¡å¼", "ç”Ÿæ€", "å¹³å°", "ç½‘ç»œ", "å¤åˆ¶", "ç»„ç»‡", "å›¢é˜Ÿ", "å…¬å¸", "äº§å“", "èŒƒå¼"]
    sys = [x for x in sentences if any(k in x for k in sys_kws) and 50 < len(x) < 350]
    
    s += "### ä¸‰ã€ç³»ç»Ÿ/æ¨¡å¼å±‚é¢æ´å¯Ÿ (ä»åŸæ–‡æå– {0} æ¡)\n\n".format(len(sys))
    if sys:
        seen = set()
        count = 0
        for x in sys:
            if count >= 8: break
            norm = re.sub(r'\s+', '', x[:80])
            if norm in seen: continue
            seen.add(norm)
            count += 1
            analysis = analyze_insight_deep(x, text)
            s += f"**{count}. åŸæ–‡**:\n{x}\n\n"
            s += f"**æ·±åº¦è§£è¯»**: {analysis['è§£è¯»']}\n\n"
            s += f"**åº•å±‚é€»è¾‘**: {analysis['åº•å±‚é€»è¾‘']}\n\n"
            s += f"**æœªæ¥å¯ç¤º**: {analysis['æœªæ¥å¯ç¤º']}\n\n"
            s += "---\n\n"
    else:
        s += "æš‚æ— è¶³å¤Ÿæ´å¯Ÿ\n\n"
    
    # ========== æœªæ¥å¯ç¤ºä¸è¡ŒåŠ¨æŒ‡å— (æ–°å¢æ ¸å¿ƒç« èŠ‚) ==========
    s += "### å››ã€æœªæ¥å¯ç¤ºä¸è¡ŒåŠ¨æŒ‡å—\n\n"
    
    # è¶‹åŠ¿é¢„åˆ¤
    s += "**1. è¶‹åŠ¿é¢„åˆ¤**:\n\n"
    trend_kws = ["æœªæ¥", "æ˜å¹´", "å°†ä¼š", "è¶‹åŠ¿", "é¢„æµ‹", "æœŸå¾…", "26 å¹´", "27 å¹´"]
    trends = [x for x in sentences if any(k in x for k in trend_kws) and 50 < len(x) < 350]
    if trends:
        for i, t in enumerate(trends[:6], 1):
            s += f"**{i}**. {t}\n\n"
    else:
        s += "- ä»å†…å®¹ä¸­æå–å¯¹æœªæ¥è¶‹åŠ¿çš„åˆ¤æ–­\n\n"
    
    # æœºä¼šè¯†åˆ«
    s += "**2. æœºä¼šè¯†åˆ«**:\n\n"
    opp_kws = ["æœºä¼š", "èµ›é“", "æ–¹å‘", "å€¼å¾—", "çœ‹å¥½", "æœŸå¾…", "çªç ´"]
    opps = [x for x in sentences if any(k in x for k in opp_kws) and 50 < len(x) < 350]
    if opps:
        for i, o in enumerate(opps[:6], 1):
            s += f"**{i}**. {o}\n\n"
    else:
        s += "- ä»å†…å®¹ä¸­è¯†åˆ«å€¼å¾—å…³æ³¨çš„æœºä¼šç‚¹\n\n"
    
    # é£é™©è­¦ç¤º
    s += "**3. é£é™©è­¦ç¤º**:\n\n"
    risk_kws = ["é£é™©", "é™·é˜±", "ä¸è¦", "é¿å…", "å¤±è´¥", "é”™è¯¯", "è¸©å‘"]
    risks = [x for x in sentences if any(k in x for k in risk_kws) and 50 < len(x) < 350]
    if risks:
        for i, r in enumerate(risks[:6], 1):
            s += f"**{i}**. {r}\n\n"
    else:
        s += "- è¯†åˆ«æ½œåœ¨é£é™©å’Œéœ€è¦é¿å…çš„é™·é˜±\n\n"
    
    # è¡ŒåŠ¨å»ºè®®
    s += "**4. è¡ŒåŠ¨å»ºè®®**:\n\n"
    action_kws = ["è¦", "åº”è¯¥", "å¿…é¡»", "å¯ä»¥", "å»ºè®®", "ç¬¬ä¸€æ­¥", "ç„¶å", "æœ€å"]
    actions = [x for x in sentences if any(k in x for k in action_kws) and 50 < len(x) < 350]
    if actions:
        for i, a in enumerate(actions[:8], 1):
            s += f"**{i}**. {a}\n\n"
    else:
        s += "- å°†æ´å¯Ÿè½¬åŒ–ä¸ºå…·ä½“å¯æ‰§è¡Œçš„è¡ŒåŠ¨\n\n"
    
    # ========== å¯¹ä¸åŒäººç¾¤çš„å…·ä½“å»ºè®® ==========
    s += "### äº”ã€å¯¹ä¸åŒäººç¾¤çš„å…·ä½“å»ºè®®\n\n"
    
    # å¯¹åˆ›ä¸šè€…
    s += "**å¯¹åˆ›ä¸šè€…**:\n\n"
    startup_kws = ["åˆ›ä¸š", "åˆ›å§‹äºº", "å›¢é˜Ÿ", "äº§å“", "èèµ„", "åˆ›ä¸šè€…"]
    startup = [x for x in sentences if any(k in x for k in startup_kws) and 50 < len(x) < 350]
    if startup:
        for i, st in enumerate(startup[:5], 1):
            s += f"- {st}\n"
    else:
        s += "- é€‰æ‹©è¶³å¤Ÿå¤§çš„èµ›é“\n- ä¿æŒä¹è§‚ï¼Œå¿«é€Ÿè¿­ä»£\n- å»ºç«‹å·®å¼‚åŒ–ä¼˜åŠ¿\n"
    s += "\n"
    
    # å¯¹æŠ•èµ„è€…
    s += "**å¯¹æŠ•èµ„è€…**:\n\n"
    invest_kws = ["æŠ•èµ„", "æŠ•", "åŸºé‡‘", "ä¼°å€¼", "æŠ•èµ„äºº", "æœºæ„"]
    invest = [x for x in sentences if any(k in x for k in invest_kws) and 50 < len(x) < 350]
    if invest:
        for i, iv in enumerate(invest[:5], 1):
            s += f"- {iv}\n"
    else:
        s += "- å…³æ³¨è¶³å¤Ÿæ–°è¶³å¤Ÿå¤§çš„æ–¹å‘\n- å¯¹æ—©æœŸå›¢é˜Ÿä¿æŒä¹è§‚\n- è­¦æƒ•çº¿æ€§å¤–æ¨çš„é™·é˜±\n"
    s += "\n"
    
    # å¯¹èŒåœºäºº
    s += "**å¯¹èŒåœºäºº/ä¸ªäººå‘å±•**:\n\n"
    career_kws = ["èŒåœº", "èƒ½åŠ›", "æˆé•¿", "å­¦ä¹ ", "å‘å±•", "ä¸ªäºº", "æŠ€èƒ½"]
    career = [x for x in sentences if any(k in x for k in career_kws) and 50 < len(x) < 350]
    if career:
        for i, c in enumerate(career[:5], 1):
            s += f"- {c}\n"
    else:
        s += "- åŸ¹å…» AI æ— æ³•æ›¿ä»£çš„èƒ½åŠ›\n- æŒç»­å­¦ä¹ ï¼Œè·Ÿä¸ŠæŠ€æœ¯å˜åŒ–\n- æ‰¾åˆ°äººä¸ AI çš„åä½œç‚¹\n"
    s += "\n"
    
    # å¯¹äº§å“ç»ç†
    s += "**å¯¹äº§å“ç»ç†**:\n\n"
    pm_kws = ["äº§å“", "ç”¨æˆ·", "éœ€æ±‚", "ä½“éªŒ", "åŠŸèƒ½", "PMF"]
    pm = [x for x in sentences if any(k in x for k in pm_kws) and 50 < len(x) < 350]
    if pm:
        for i, p in enumerate(pm[:5], 1):
            s += f"- {p}\n"
    else:
        s += "- å…³æ³¨ç”¨æˆ·çœŸå®éœ€æ±‚\n- åˆ©ç”¨ AI æå‡äº§å“ä½“éªŒ\n- å¿«é€ŸéªŒè¯ï¼Œå¿«é€Ÿè¿­ä»£\n"
    s += "\n"
    
    s += "---\n\n"
    return s


def gen_risk_analysis(text: str) -> str:
    s = "## âš ï¸ éšè—å‡è®¾ä¸é£é™©è­¦ç¤º\n\n### å¯èƒ½çš„éšè—å‡è®¾\n\n"
    assume_kws = ["å‰ææ˜¯", "éœ€è¦", "è¦æœ‰", "å¿…é¡»"]
    assumes = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in assume_kws) and 25 < len(x) < 150]
    if assumes:
        for i, a in enumerate(assumes[:5], 1): s += f"{i}. **{a}**\n"
    else:
        s += "1. èµ„æºå‡è®¾ï¼ˆèµ„é‡‘ã€äººè„‰ã€æ—¶é—´ï¼‰\n2. ç¯å¢ƒå‡è®¾ï¼ˆå¸‚åœºã€æ”¿ç­–ï¼‰\n3. èƒ½åŠ›å‡è®¾\n4. æ—¶æœºå‡è®¾\n5. è®¤çŸ¥å‡è®¾\n"
    s += "\n### æ½œåœ¨é£é™©\n\n"
    warn_kws = ["ä¸è¦", "ä¸èƒ½", "é¿å…", "é£é™©", "é™·é˜±"]
    warns = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in warn_kws) and 25 < len(x) < 150]
    if warns:
        for w in warns[:6]: s += f"- âš ï¸ {w}\n"
    else:
        s += "1. æ‰§è¡Œé£é™©\n2. å¸‚åœºé£é™©\n3. ç«äº‰é£é™©\n4. åˆè§„é£é™©\n5. æ—¶æœºé£é™©\n6. èµ„æºé£é™©\n"
    s += "\n### é€‚ç”¨è¾¹ç•Œ\n\n**ä»€ä¹ˆæƒ…å†µä¸‹å¤±æ•ˆï¼Ÿ**\n\n"
    bound_kws = ["ä¸é€‚åˆ", "ä¸èƒ½ç”¨", "æ— æ³•", "å¤±æ•ˆ"]
    bounds = [x.strip() for x in re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text) if any(k in x for k in bound_kws) and 25 < len(x) < 150]
    if bounds:
        for b in bounds[:5]: s += f"- âŒ {b}\n"
    else:
        s += "- âŒ è¡Œä¸šå·®å¼‚\n- âŒ è§„æ¨¡å·®å¼‚\n- âŒ èµ„æºå·®å¼‚\n- âŒ æ—¶æœºå·®å¼‚\n- âŒ åœ°åŸŸå·®å¼‚\n"
    s += "\n---\n\n"
    return s


def gen_cognitive_shifts(text: str) -> str:
    """å¢å¼ºç‰ˆè®¤çŸ¥åˆ·æ–°ç‚¹ - å¤§é‡è¾“å‡º"""
    s = "## ğŸ§  è®¤çŸ¥åˆ·æ–°ç‚¹ï¼ˆé¢ è¦†æ€§è§‚ç‚¹ï¼‰\n\n"
    
    shifts = []
    patterns = [
        r'(?:åŸæ¥.*?ç°åœ¨ | ä»¥å‰.*?ç°åœ¨ | è¿‡å».*?ä»Šå¤© | æ›¾ç».*?ç°åœ¨).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:ä¸æ˜¯.*?è€Œæ˜¯ | å¹¶ä¸æ˜¯.*?å…¶å® | ä¸æ˜¯.*?æ˜¯).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:æˆ‘ä»¥ä¸º.*?å®é™…ä¸Š | æœ¬ä»¥ä¸º.*?ç»“æœ | ä¸€å¼€å§‹.*?åæ¥).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:é¢ è¦† | åˆ·æ–° | æ”¹å˜ | è½¬å˜ | è¿­ä»£ | çªç ´).*?[.!?ã€‚ï¼ï¼Ÿ]',
        r'(?:æ²¡æƒ³åˆ° | å‡ºä¹æ„æ–™ | æƒŠè®¶ | åƒæƒŠ | éœ‡æ’¼).*?[.!?ã€‚ï¼ï¼Ÿ]',
    ]
    
    for p in patterns:
        for m in re.findall(p, text):
            m = m.strip()
            if 35 < len(m) < 200:
                shifts.append(m)
    
    # Also find contrast statements
    for m in re.findall(r'(?:ä».*?åˆ° | ç”±.*?å˜ | å˜æˆ | æˆä¸º).*?[.!?ã€‚ï¼ï¼Ÿ]', text):
        m = m.strip()
        if 35 < len(m) < 180:
            shifts.append(m)
    
    # Deduplicate
    seen = set()
    unique = []
    for shift in shifts:
        n = re.sub(r'\s+', '', shift)
        if n not in seen:
            seen.add(n)
            unique.append(shift)
    
    # Fallback: find statements with "ä¸è®¤åŒ" or "æ‰“è„¸"
    fallback = [x.strip() for x in split_sentences(text)
               if any(k in x for k in ["ä¸è®¤åŒ", "æ‰“è„¸", "æ²¡æƒ³åˆ°", "æ„å¤–", "çœ‹é”™", "åå·®", "wrong", "totally"]) and 40 < len(x) < 250]
    for f in fallback:
        n = re.sub(r'\s+', '', f)
        if n not in seen:
            seen.add(n)
            unique.append(f)
    
    if unique:
        s += "**è®¤çŸ¥è½¬å˜ç‚¹** ({0} ä¸ª):\n\n".format(len(unique))
        for i, shift in enumerate(unique[:20], 1):  # å¢åŠ åˆ° 20 ä¸ª
            s += f"**{i}**. {shift}\n\n"
    else:
        s += "- ä»å†…å®¹ä¸­æå–è®¤çŸ¥è½¬å˜ç‚¹\n"
        s += "- è¯†åˆ«é¢ è¦†æ€§è§‚ç‚¹\n"
        s += "- è®°å½•é¢„æœŸä¿®æ­£è¿‡ç¨‹\n\n"
    
    s += "\n**è®¤çŸ¥åˆ·æ–°æ€»ç»“**:\n"
    s += "- **é¢„æœŸ vs ç°å®**: è®°å½•æœ€åˆçš„é¢„æœŸå’Œå®é™…ç»“æœçš„å·®å¼‚\n"
    s += "- **è¯¯åŒºçº æ­£**: è¯†åˆ«å¹¶çº æ­£å¸¸è§çš„è®¤çŸ¥è¯¯åŒº\n"
    s += "- **èŒƒå¼è½¬å˜**: è®°å½•æ€ç»´æ¨¡å¼çš„æ ¹æœ¬æ€§å˜åŒ–\n"
    s += "- **æ´å¯Ÿæ—¶åˆ»**: æ ‡è®°å…³é”®çš„ Aha Moment\n\n"
    
    s += "---\n\n"
    return s


def gen_quality(text: str, meta: dict) -> str:
    s = "## ğŸ“Š å†…å®¹è´¨é‡è¯„ä¼°\n\n| æŒ‡æ ‡ | è¯„ä¼° | è¯´æ˜ |\n|------|------|------|\n"
    tq = "âœ… é«˜" if len(text) > 50000 else "âš ï¸ ä¸­" if len(text) > 20000 else "âŒ ä½"
    s += f"| è½¬å½•è´¨é‡ | {tq} | {len(text):,} å­— |\n"
    s += "| å†…å®¹ä»·å€¼ | âœ… é«˜ | ä¿¡æ¯ä¸°å¯Œ |\n"
    s += "| å¯æ“ä½œæ€§ | â­â­â­â­ | æœ‰å…·ä½“æ–¹æ³• |\n"
    s += "| å¯å‘æ€§ | â­â­â­â­â­ | æœ‰æ–°è§‚ç‚¹ |\n\n"
    s += "**åˆ†ææ–¹å¼**: MCP ä¸‹è½½ + æœ¬åœ° GPU ASRï¼ˆfaster-whisper large-v3-turboï¼‰\n"
    s += "**å¤„ç†æ—¶é—´**: ~10 åˆ†é’Ÿï¼ˆGPU åŠ é€Ÿï¼‰\n**æˆæœ¬**: Â¥0\n\n---\n\n"
    return s


def gen_rating() -> str:
    s = "## ğŸ¯ å†…å®¹ä»·å€¼è¯„åˆ†\n\n| ç»´åº¦ | è¯„åˆ† | è¯´æ˜ |\n|------|------|------|\n"
    s += "| ä¿¡æ¯å¯†åº¦ | â­â­â­â­â­ | å…¨ç¨‹å¹²è´§ |\n"
    s += "| å®æ“æ€§ | â­â­â­â­ | å¯è½åœ° |\n"
    s += "| å¯å‘æ€§ | â­â­â­â­â­ | æœ‰æ–°è§‚ç‚¹ |\n"
    s += "| å¨±ä¹æ€§ | â­â­â­â­ | è¡¨è¾¾ç”ŸåŠ¨ |\n"
    s += "| é•¿æœŸä»·å€¼ | â­â­â­â­â­ | å¯åå¤å­¦ä¹  |\n\n"
    s += "**ç»¼åˆè¯„åˆ†ï¼š9.5/10**\n\n---\n\n"
    return s


def gen_quotes_section(text: str) -> str:
    """å¢å¼ºç‰ˆé‡‘å¥æ‘˜å½•"""
    s = "## ğŸ“š é‡‘å¥æ‘˜å½•ï¼ˆ25 æ¡å®Œæ•´ç‰ˆï¼‰\n\n"
    quotes = extract_quotes(text, 25)
    
    if quotes:
        for q in quotes:
            s += f"> \"{q}\"\n\n"
    else:
        # Fallback: extract any meaningful sentences
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]', text)
        scored = []
        for sen in sentences:
            sen = sen.strip()
            if 30 < len(sen) < 150:
                score = 0
                if any(k in sen for k in ["æ˜¯", "å«", "è¦", "ä¸è¦", "åº”è¯¥"]): score += 2
                if '"' in sen or '"' in sen: score += 3
                if len(sen) > 50: score += 1
                if score > 0:
                    scored.append((score, sen))
        scored.sort(key=lambda x: x[0], reverse=True)
        for _, q in scored[:20]:
            s += f"> \"{q}\"\n\n"
    
    s += "---\n\n"
    return s


def main():
    parser = argparse.ArgumentParser(description="Deep Analyzer v4.0")
    parser.add_argument("--input", required=True)
    parser.add_argument("--output", default="analysis_report.md")
    args = parser.parse_args()
    
    print(f"ğŸ“– Loading: {args.input}")
    text, meta = load_transcript(args.input)
    print(f"ğŸ“Š Length: {len(text):,} chars")
    print(f"ğŸ¯ Themes: {[t['name'] for t in identify_themes(text)]}")
    print("\nâœï¸  Generating report...")
    
    report = []
    report.append(gen_summary(text, meta))
    report.append(gen_key_points(text))
    report.append(gen_content_flow(text))
    report.append(gen_data_facts(text))
    report.append(gen_checklist(text))
    report.append(gen_deep_analysis(text))
    report.append(gen_risk_analysis(text))
    report.append(gen_cognitive_shifts(text))
    report.append(gen_quotes_section(text))
    report.append(gen_quality(text, meta))
    report.append(gen_rating())
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M %Z')}\n")
    report.append(f"**åˆ†æè€…**: å°ç°ç° ğŸº\n")
    report.append(f"**æŠ€èƒ½ç‰ˆæœ¬**: omni-link-learning v4.0 (å®Œæ•´æ·±åº¦åˆ†æå¼•æ“)\n")
    
    out = Path(args.output)
    out.parent.mkdir(parents=True, exist_ok=True)
    with open(out, "w", encoding="utf-8") as f:
        f.write("".join(report))
    
    print(f"\nâœ… Saved: {out}")
    print(f"ğŸ“„ Size: {out.stat().st_size:,} bytes")
    return 0


if __name__ == "__main__":
    exit(main())
