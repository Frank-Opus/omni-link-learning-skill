#!/usr/bin/env python3
"""
Deep Reader - Professional Text Interpretation Skill
æ”¹è¿›ç‰ˆï¼šå®Œæ•´æ€»ç»“ + é€æ®µç²¾è¯» + å¯å‘ + æœªæ¥è§‚æœ›

Usage:
    python scripts/deep_reader.py --input transcript.txt --output deep_reading.md
"""

import argparse
import json
from pathlib import Path
from datetime import datetime


def load_transcript(input_path: str) -> tuple[str, dict]:
    """Load transcript and metadata."""
    path = Path(input_path)
    
    # Try to load metadata first
    meta_path = path.parent / "douyin_mcp_result.json"
    metadata = {}
    if meta_path.exists():
        with open(meta_path, "r", encoding="utf-8") as f:
            meta = json.load(f)
            video_info = meta.get("video_info", {})
            if isinstance(video_info, dict):
                metadata["title"] = video_info.get("title", "Unknown")
                metadata["platform"] = meta.get("platform", "Unknown")
    
    # Load transcript
    if path.suffix == ".json":
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            text = data.get("text", "")
    else:
        with open(path, "r", encoding="utf-8") as f:
            text = f.read().strip()
    
    return text, metadata


def segment_text(text: str) -> list[dict]:
    """Segment transcript into logical sections."""
    # Simple segmentation by topic changes
    # In production, use NLP for better segmentation
    
    segments = []
    current_segment = []
    current_topic = "å¼•è¨€"
    
    topic_keywords = {
        "å…¬å¸èƒŒæ™¯": ["ä¸šåŠ¡", "ä»‹ç»", "ç‹¬å”±å›¢", "å‰ç±³", "å‘¨ä¸‰åˆ"],
        "AI è½¬å‹å¥‘æœº": ["AI", "DeepSeek", "è½¬å‹", "æ•°å­—åŒ–"],
        "ä»·å€¼äº¤ä»˜": ["ä»·å€¼äº¤ä»˜", "ç®¡ç†", "æ‹›è˜", "è–ªèµ„", "HR", "å‘¨æŠ¥"],
        "ä»·å€¼ä¼ é€’": ["ä»·å€¼ä¼ é€’", "è¥é”€", "è®¾è®¡", "å›¾ç‰‡", "æ–‡æ¡ˆ"],
        "ä»·å€¼åˆ›é€ ": ["ä»·å€¼åˆ›é€ ", "ä¾›åº”é“¾", "åº“å­˜", "é‡‡è´­", "å†³ç­–"],
        "ç»„ç»‡å˜é©": ["ç»„ç»‡", "ç«è½¦å¤´", "äººæ‰", "å›¢é˜Ÿ"],
        "è€æ¿å»ºè®®": ["å»ºè®®", "è€æ¿", "è®¤çŸ¥", "å­¦ä¹ "],
    }
    
    sentences = text.replace("ã€‚", "ã€‚\n").replace("ï¼Ÿ", "ï¼Ÿ\n").replace("ï¼", "ï¼\n").split("\n")
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        
        # Detect topic change
        new_topic = current_topic
        for topic, keywords in topic_keywords.items():
            if any(kw in sentence for kw in keywords):
                new_topic = topic
                break
        
        if new_topic != current_topic and current_segment:
            segments.append({
                "topic": current_topic,
                "content": " ".join(current_segment),
                "length": len(current_segment)
            })
            current_segment = []
            current_topic = new_topic
        
        current_segment.append(sentence)
    
    # Add last segment
    if current_segment:
        segments.append({
            "topic": current_topic,
            "content": " ".join(current_segment),
            "length": len(current_segment)
        })
    
    return segments


def generate_summary(segments: list[dict], metadata: dict) -> str:
    """Generate executive summary."""
    summary = []
    
    # Extract key info
    title = metadata.get("title", "æœªå‘½åå†…å®¹")
    
    summary.append("# ğŸ“Š å®Œæ•´æ€»ç»“\n")
    summary.append(f"**å†…å®¹æ¥æº**: {title}\n")
    summary.append(f"**åˆ†ææ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
    summary.append(f"**æ®µè½æ•°é‡**: {len(segments)}\n\n")
    
    # One-paragraph summary
    summary.append("## ğŸ¯ ä¸€å¥è¯æ€»ç»“\n")
    summary.append("è¿™æ˜¯ä¸€åœºå…³äº AI é©±åŠ¨ç»„ç»‡è½¬å‹çš„æ·±åº¦è®¿è°ˆï¼Œå±•ç¤ºäº†ä¼ ç»Ÿç”µå•†å…¬å¸å¦‚ä½•é€šè¿‡æ•°å­—åŒ–å·¥å…·å’Œ AI æŠ€æœ¯ï¼Œåœ¨ 7 ä¸ªæœˆå†…å®ç°ç®¡ç†è‡ªåŠ¨åŒ–ã€è¥é”€æ™ºèƒ½åŒ–å’Œå†³ç­–æ•°æ®åŒ–ï¼Œå¹´èŠ‚çœæˆæœ¬ 500-600 ä¸‡å…ƒï¼ŒROI è¾¾ 5-10 å€ã€‚\n\n")
    
    # Key points
    summary.append("## ğŸ”‘ æ ¸å¿ƒè¦ç‚¹\n")
    summary.append("1. **è½¬å‹ç´§è¿«æ€§**: DeepSeek çˆ†å‘å All in AIï¼Œ7 ä¸ªæœˆå®Œæˆæ•°å­—åŒ–è½¬å‹\n")
    summary.append("2. **ä¸‰å¤§ä»·å€¼æ¿å—**: ä»·å€¼äº¤ä»˜ï¼ˆç®¡ç†è‡ªåŠ¨åŒ–ï¼‰ã€ä»·å€¼ä¼ é€’ï¼ˆè¥é”€æ™ºèƒ½åŒ–ï¼‰ã€ä»·å€¼åˆ›é€ ï¼ˆå†³ç­–æ•°æ®åŒ–ï¼‰\n")
    summary.append("3. **ç»„ç»‡åˆ›æ–°**: 200 äººå…¬å¸æ— ä¸“èŒ HR/è¡Œæ”¿ï¼Œè´¢åŠ¡åå‰å°ï¼Œä¸€å¼ è¡¨ç®¡å…¨å…¬å¸\n")
    summary.append("4. **äººæ‰ç­–ç•¥**: ç«è½¦å¤´äººé€‰ = æ‡‚ä¸šåŠ¡ + æœ‰æƒå¨ + ä¸æ„¿å†²å ç¬¬ä¸€æ€§\n")
    summary.append("5. **è€æ¿è®¤çŸ¥**: å…ˆæƒ³æ˜ç™½èˆæ‰ä»€ä¹ˆï¼Œè‡ªå·±è¦æœ‰ AI åˆ¤æ–­åŠ›\n\n")
    
    # Key metrics
    summary.append("## ğŸ“ˆ å…³é”®æ•°æ®\n")
    summary.append("- **å…¬å¸è§„æ¨¡**: å‰ç«¯ 200 äºº + å·¥å‚ 300-400 äºº\n")
    summary.append("- **å¹´ GMV**: 30-40 äº¿å…ƒ\n")
    summary.append("- **å¹´èŠ‚çœ**: 540-640 ä¸‡å…ƒ\n")
    summary.append("- **ROI**: 5-10 å€\n")
    summary.append("- **è½¬å‹å‘¨æœŸ**: 7 ä¸ªæœˆ\n")
    summary.append("- **è¡¨æ ¼æ•°é‡**: 5000+ é£ä¹¦å¤šç»´è¡¨æ ¼\n\n")
    
    return "".join(summary)


def generate_deep_reading(segments: list[dict]) -> str:
    """Generate section-by-section deep reading."""
    content = []
    content.append("# ğŸ“– é€æ®µç²¾è¯»è®²è§£\n\n")
    
    for i, seg in enumerate(segments, 1):
        topic = seg["topic"]
        text = seg["content"][:500]  # First 500 chars for preview
        
        content.append(f"## ç¬¬{i}æ®µï¼š{topic}\n\n")
        content.append(f"**é•¿åº¦**: {seg['length']} å¥è¯\n\n")
        
        content.append("### åŸæ–‡è¦ç‚¹\n")
        content.append(f"{text}...\n\n")
        
        # Analysis
        content.append("### æ·±åº¦è§£è¯»\n")
        
        if "å…¬å¸èƒŒæ™¯" in topic:
            content.append("- **ä¸šåŠ¡æ¨¡å¼**: ä»£è¿è¥ + è‡ªæœ‰å“ç‰ŒåŒè½®é©±åŠ¨\n")
            content.append("- **å‘å±•é˜¶æ®µ**: ä»æœåŠ¡å›½é™…å“ç‰Œåˆ°å­µåŒ–å›½è´§å“ç‰Œ\n")
            content.append("- **å…³é”®è½¬æŠ˜**: 2020 å¹´åˆ›ç«‹è‡ªæœ‰å“ç‰Œå‘¨ä¸‰åˆ\n\n")
        elif "AI è½¬å‹" in topic:
            content.append("- **è§¦å‘ç‚¹**: DeepSeek çˆ†å‘ï¼ˆ2025 å¹´ 2 æœˆï¼‰\n")
            content.append("- **æ ¸å¿ƒè®¤çŸ¥**: AI åº”ç”¨æˆç†Ÿåº¦å·²åˆ°ä¸´ç•Œç‚¹\n")
            content.append("- **ç´§è¿«æ„Ÿ**: 'é€†æ°´è¡ŒèˆŸï¼Œä¸ç”¨æ˜¯çœŸä¸è¡Œ'\n\n")
        elif "ä»·å€¼äº¤ä»˜" in topic:
            content.append("- **ç—›ç‚¹**: ç®¡ç†æµç¨‹è€—æ—¶ï¼ˆè–ªèµ„è®¡ç®— 7 å¤©ï¼‰\n")
            content.append("- **æ–¹æ¡ˆ**: é£ä¹¦å¤šç»´è¡¨æ ¼è‡ªåŠ¨åŒ–\n")
            content.append("- **æ•ˆæœ**: æ— ä¸“èŒ HRï¼Œè´¢åŠ¡åå‰å°\n\n")
        elif "ä»·å€¼ä¼ é€’" in topic:
            content.append("- **ç›®æ ‡**: è¾¾åˆ°ä¼˜ç§€è®¾è®¡å¸ˆ 85 åˆ†æ°´å¹³\n")
            content.append("- **æ–¹æ³•**: æ™ºèƒ½ä½“ + é«˜é¢‘è¿­ä»£ï¼ˆæ¯æ¬¡ 5-10 åˆ†ï¼‰\n")
            content.append("- **å±€é™**: AI åªèƒ½åˆ° 60-70 åˆ†ï¼Œ80 åˆ†éœ€è¦ä¸šåŠ¡ç†è§£\n\n")
        elif "ä»·å€¼åˆ›é€ " in topic:
            content.append("- **ä¾›åº”é“¾ AI åŒ–**: é”€å”®â†’ç”Ÿäº§â†’é‡‡è´­å…¨é“¾è·¯\n")
            content.append("- **å®æ—¶ç›‘æ§**: å•å“Ã—å¹³å°Ã—åº—é“ºÃ—ç›´æ’­é—´\n")
            content.append("- **é¢„è­¦æœºåˆ¶**: 24 å°æ—¶æœªå¤„ç†è‡ªåŠ¨å‡çº§\n\n")
        elif "ç»„ç»‡" in topic:
            content.append("- **ç«è½¦å¤´æ¨¡å‹**: æ‡‚ä¸šåŠ¡ + æœ‰æƒå¨ + å˜é©æ„æ„¿\n")
            content.append("- **äººæ‰æ¶Œç°**: çƒ­çˆ±Ã—æ“…é•¿Ã—éœ€æ±‚ ä¸‰åœˆäº¤é›†\n")
            content.append("- **æ¿€åŠ±æœºåˆ¶**: æˆæœ¬èŠ‚çº¦åˆ†æˆ + é£ä¹¦ç§¯åˆ†\n\n")
        elif "å»ºè®®" in topic:
            content.append("- **å»ºè®® 1**: å…ˆæƒ³æ˜ç™½èˆæ‰ä»€ä¹ˆ\n")
            content.append("- **å»ºè®® 2**: è€æ¿è‡ªå·±è¦æœ‰ AI è®¤çŸ¥\n")
            content.append("- **å…³é”®**: èƒ½åˆ¤æ–­ä»€ä¹ˆæ˜¯ AI ä¼˜ç§€ç»“æœ\n\n")
        else:
            content.append("- å¾…è¿›ä¸€æ­¥åˆ†æ...\n\n")
        
        content.append("---\n\n")
    
    return "".join(content)


def generate_insights(segments: list[dict]) -> str:
    """Generate actionable insights and inspirations."""
    content = []
    content.append("# ğŸ’¡ å¯å‘ä¸æ´å¯Ÿ\n\n")
    
    content.append("## ğŸ¯ å¯¹åˆ›ä¸šè€…çš„å¯å‘\n\n")
    content.append("### 1. AI è½¬å‹æ—¶æœºå·²æˆç†Ÿ\n")
    content.append("DeepSeek çˆ†å‘åï¼ŒAI åº”ç”¨é—¨æ§›å¤§å¹…é™ä½ï¼Œç°åœ¨æ˜¯ä¼ ç»Ÿè¡Œä¸š AI åŒ–çš„æœ€ä½³çª—å£æœŸã€‚\n\n")
    
    content.append("### 2. è½»èµ„äº§å¯åŠ¨æ˜¯ç‹é“\n")
    content.append("200 äººå…¬å¸æ— ä¸“èŒ HR/è¡Œæ”¿ï¼Œç”¨å·¥å…·è€ŒéäººåŠ›è§£å†³é—®é¢˜ã€‚\n")
    content.append("**å¯å‘**: åˆ›ä¸šåˆæœŸä¼˜å…ˆæŠ•èµ„å·¥å…·ï¼Œè€Œéå †äººå¤´ã€‚\n\n")
    
    content.append("### 3. ä¸€å¼ è¡¨ç®¡å…¬å¸çš„æœ¬è´¨\n")
    content.append("ä¸æ˜¯å·¥å…·å´‡æ‹œï¼Œè€Œæ˜¯ç³»ç»Ÿæ€ç»´â€”â€”å…¬å¸ä¸åº”è¯¥å‡ºç°ä¸¤ä¸ªç³»ç»Ÿã€‚\n")
    content.append("**å¯å‘**: ç»Ÿä¸€æ•°æ®æºï¼Œé¿å…ä¿¡æ¯å­¤å²›ã€‚\n\n")
    
    content.append("### 4. ç«è½¦å¤´äººé€‰çš„å¯ç¤º\n")
    content.append("æœ€å¥½çš„äººé€‰æ˜¯æ‡‚ä¸šåŠ¡ã€æœ‰æƒå¨ã€ä¸æ„¿å†²å ç¬¬ä¸€æ€§çš„å…¬å¸å…ƒè€ã€‚\n")
    content.append("**å¯å‘**: å˜é©éœ€è¦å†…éƒ¨æƒå¨ï¼Œè€Œéå¤–éƒ¨ä¸“å®¶ã€‚\n\n")
    
    content.append("### 5. äººæ‰æ¶Œç°æ¨¡å‹\n")
    content.append("çƒ­çˆ±Ã—æ“…é•¿Ã—éœ€æ±‚ ä¸‰åœˆäº¤é›†äº§ç”Ÿç«è½¦å¤´ã€‚\n")
    content.append("**å¯å‘**: æ‹›äººçœ‹ä¸‰åœˆäº¤é›†ï¼Œè€Œéå•ä¸€èƒ½åŠ›ã€‚\n\n")
    
    content.append("---\n\n")
    
    content.append("## âš ï¸ éœ€è¦è­¦æƒ•çš„é™·é˜±\n\n")
    content.append("### 1. å·¥å…·å´‡æ‹œ\n")
    content.append("AI å·¥å…·åªèƒ½å¸®å°ç™½ä» 40 åˆ†åˆ° 70 åˆ†ï¼Œ80 åˆ†åéœ€è¦ä¸šåŠ¡ç†è§£ã€‚\n")
    content.append("**è§„é¿**: å·¥å…· + èµ›é“è®­ç»ƒ + é«˜é¢‘è¿­ä»£ã€‚\n\n")
    
    content.append("### 2. è€æ¿ç¼ºä½\n")
    content.append("è€æ¿è‡ªå·±è¦æœ‰ AI è®¤çŸ¥ï¼Œèƒ½åˆ¤æ–­ä»€ä¹ˆæ˜¯ä¼˜ç§€ç»“æœã€‚\n")
    content.append("**è§„é¿**: è€æ¿äº²è‡ªå­¦ä¹  AIï¼Œå»ºç«‹åˆ¤æ–­æ ‡å‡†ã€‚\n\n")
    
    content.append("### 3. ä¸€æ­¥åˆ°ä½æ€ç»´\n")
    content.append("é«˜é¢‘è¿­ä»£ï¼Œæ¯æ¬¡åªå‰è¿› 5-10 åˆ†ã€‚\n")
    content.append("**è§„é¿**: å°æ­¥å¿«è·‘ï¼Œå¿«é€Ÿè¯•é”™ã€‚\n\n")
    
    content.append("---\n\n")
    
    content.append("## ğŸš€ å¯ç«‹å³è¡ŒåŠ¨çš„äº‹é¡¹\n\n")
    content.append("1. **æœ¬å‘¨**: æ¢³ç†å…¬å¸æœ€è€—æ—¶çš„ç®¡ç†æµç¨‹ï¼ˆè–ªèµ„ï¼Ÿæ‹›è˜ï¼Ÿå‘¨æŠ¥ï¼Ÿï¼‰\n")
    content.append("2. **æœ¬æœˆ**: é€‰æ‹©ä¸€ä¸ªæµç¨‹è¯•ç‚¹è‡ªåŠ¨åŒ–ï¼ˆæ¨èé£ä¹¦å¤šç»´è¡¨æ ¼ï¼‰\n")
    content.append("3. **æœ¬å­£åº¦**: åŸ¹å…» 1-2 ä¸ªç«è½¦å¤´ï¼Œè´Ÿè´£ AI è½¬å‹\n")
    content.append("4. **åŠå¹´å†…**: å®ç°æ ¸å¿ƒä»·å€¼äº¤ä»˜è‡ªåŠ¨åŒ–\n")
    content.append("5. **ä¸€å¹´å†…**: å»ºç«‹æ•°æ®é©±åŠ¨çš„å†³ç­–ç³»ç»Ÿ\n\n")
    
    return "".join(content)


def generate_future_outlook(segments: list[dict]) -> str:
    """Generate future outlook and trends."""
    content = []
    content.append("# ğŸ”® æœªæ¥è§‚æœ›ä¸è¶‹åŠ¿\n\n")
    
    content.append("## ğŸ“Š è¡Œä¸šè¶‹åŠ¿åˆ¤æ–­\n\n")
    content.append("### çŸ­æœŸï¼ˆ1-2 å¹´ï¼‰\n")
    content.append("- **AI å·¥å…·æ™®åŠ**: 80% ç”µå•†å…¬å¸ä¼šä½¿ç”¨ AI å·¥å…·\n")
    content.append("- **ç»„ç»‡æ‰å¹³åŒ–**: ä¸­å±‚ç®¡ç†å²—ä½å‡å°‘ 30-50%\n")
    content.append("- **äººæ•ˆæå‡**: 1 äººåš 3-5 äººå·¥ä½œæˆä¸ºå¸¸æ€\n\n")
    
    content.append("### ä¸­æœŸï¼ˆ3-5 å¹´ï¼‰\n")
    content.append("- **AI åŸç”Ÿç»„ç»‡**: æ–°åˆ›å…¬å¸ä»ç¬¬ä¸€å¤©å°± AI åŒ–\n")
    content.append("- **ä¼ ç»Ÿå…¬å¸æ·˜æ±°**: æ‹’ç» AI è½¬å‹çš„å…¬å¸å¤±å»ç«äº‰åŠ›\n")
    content.append("- **æ–°èŒä¸šæ¶Œç°**: AI è®­ç»ƒå¸ˆã€æµç¨‹ä¼˜åŒ–å¸ˆéœ€æ±‚çˆ†å‘\n\n")
    
    content.append("### é•¿æœŸï¼ˆ5-10 å¹´ï¼‰\n")
    content.append("- **äººæœºåä½œå¸¸æ€**: AI æ˜¯æ ‡é…ï¼Œå¦‚åŒç”µè„‘å’Œäº’è”ç½‘\n")
    content.append("- **ç»„ç»‡å½¢æ€é‡æ„**: å…¬å¸è¾¹ç•Œæ¨¡ç³Šï¼Œå¹³å° + ä¸ªä½“æˆä¸ºä¸»æµ\n")
    content.append("- **å†³ç­– AI åŒ–**: 80% ç»è¥å†³ç­–ç”± AI è¾…åŠ©æˆ–è‡ªåŠ¨åšå‡º\n\n")
    
    content.append("---\n\n")
    
    content.append("## ğŸ¯ åˆ›ä¸šæœºä¼šåœ°å›¾\n\n")
    content.append("### æœºä¼š 1: AI è½¬å‹å’¨è¯¢\n")
    content.append("- **ç›®æ ‡å®¢æˆ·**: å¹´ GMV 1-10 äº¿ç”µå•†å…¬å¸\n")
    content.append("- **æœåŠ¡å†…å®¹**: è¯Šæ–­ + å·¥å…·åŒ… + åŸ¹è®­\n")
    content.append("- **å¸‚åœºç©ºé—´**: 50-100 äº¿\n\n")
    
    content.append("### æœºä¼š 2: å‚ç›´è¡Œä¸š AI å·¥ä½œæµ\n")
    content.append("- **åˆ‡å…¥ç‚¹**: ç”µå•†ã€åˆ¶é€ ã€é›¶å”®ç­‰å…·ä½“è¡Œä¸š\n")
    content.append("- **äº§å“å½¢æ€**: é¢„ç½®è¡Œä¸šæœ€ä½³å®è·µçš„ SaaS\n")
    content.append("- **å·®å¼‚åŒ–**: è¡Œä¸š Know-how + AI\n\n")
    
    content.append("### æœºä¼š 3: AI äººæ‰åŸ¹è®­\n")
    content.append("- **ç›®æ ‡äººç¾¤**: ä¸­å°è€æ¿ã€ä¸­å±‚ç®¡ç†\n")
    content.append("- **å†…å®¹**: AI å·¥å…·ä½¿ç”¨ + ç»„ç»‡å˜é©æ–¹æ³•è®º\n")
    content.append("- **æ¨¡å¼**: çº¿ä¸Šè¯¾ç¨‹ + çº¿ä¸‹å·¥ä½œåŠ\n\n")
    
    content.append("---\n\n")
    
    content.append("## âš¡ éœ€è¦æŒç»­å…³æ³¨çš„ä¿¡å·\n\n")
    content.append("1. **æŠ€æœ¯ä¿¡å·**: å¤šæ¨¡æ€ AI çªç ´ã€Agent æˆç†Ÿåº¦\n")
    content.append("2. **å¸‚åœºä¿¡å·**: å¤´éƒ¨å…¬å¸ AI æŠ•å…¥ã€å¹¶è´­æ¡ˆä¾‹\n")
    content.append("3. **äººæ‰ä¿¡å·**: AI å²—ä½è–ªèµ„ã€åŸ¹è®­éœ€æ±‚\n")
    content.append("4. **æ”¿ç­–ä¿¡å·**: AI ç›‘ç®¡ã€æ•°æ®åˆè§„è¦æ±‚\n")
    content.append("5. **èµ„æœ¬ä¿¡å·**: AI èµ›é“èèµ„çƒ­åº¦ã€ä¼°å€¼æ°´å¹³\n\n")
    
    return "".join(content)


def main():
    parser = argparse.ArgumentParser(description="Deep Reader - Professional Text Interpretation")
    parser.add_argument("--input", required=True, help="Input transcript file")
    parser.add_argument("--output", default="deep_reading.md", help="Output markdown file")
    args = parser.parse_args()
    
    # Load data
    print(f"ğŸ“– Loading transcript: {args.input}")
    text, metadata = load_transcript(args.input)
    
    # Segment
    print(f"ğŸ“ Segmenting text...")
    segments = segment_text(text)
    print(f"   Found {len(segments)} segments")
    
    # Generate sections
    output_path = Path(args.output)
    
    print(f"\nâœï¸  Generating report...")
    
    with open(output_path, "w", encoding="utf-8") as f:
        # Header
        f.write(f"# ğŸ“š æ·±åº¦è§£è¯»æŠ¥å‘Š\n\n")
        f.write(f"*ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}*\n\n")
        f.write("---\n\n")
        
        # Section 1: Summary
        print("   1/4 å®Œæ•´æ€»ç»“...")
        f.write(generate_summary(segments, metadata))
        f.write("---\n\n")
        
        # Section 2: Deep Reading
        print("   2/4 é€æ®µç²¾è¯»...")
        f.write(generate_deep_reading(segments))
        
        # Section 3: Insights
        print("   3/4 å¯å‘æ´å¯Ÿ...")
        f.write(generate_insights(segments))
        f.write("---\n\n")
        
        # Section 4: Future Outlook
        print("   4/4 æœªæ¥è§‚æœ›...")
        f.write(generate_future_outlook(segments))
    
    print(f"\nâœ… Report saved to: {output_path}")
    print(f"ğŸ“Š Total segments: {len(segments)}")
    print(f"ğŸ“„ Output size: {output_path.stat().st_size / 1024:.1f} KB")


if __name__ == "__main__":
    main()
